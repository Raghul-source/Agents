{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97ad0c53-ae4f-47cb-988f-b016d4955f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # take environment variables from .env.\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef54b7a2-359d-4991-9d2c-2d0a4e206f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function execution result: {'location': 'Bengaluru', 'temperature': '21°C'}\n",
      "Model’s natural language reply:\n",
      "The current temperature in Bengaluru is 21°C.\n"
     ]
    }
   ],
   "source": [
    "# In this code , i have implemented a function calling when gemini api detects when a function call is needed \n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "# Define the function declaration, we are saying the function structue(i.e  function's name, what is does , what parameters it expects, their types,\n",
    "# descriptions, and which are required for the model to know how to \"call\" your function.\n",
    "weather_function = {\n",
    "    \"name\": \"get_current_temperature\",\n",
    "    \"description\": \"Gets the current temperature for a given location.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"location\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The city name, e.g. San Francisco\",\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"location\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "# implementing the function \n",
    "def get_current_temperature(location):\n",
    "    return {\"location\": location, \"temperature\": \"21°C\"}\n",
    "\n",
    "# Client setup\n",
    "client = genai.Client()\n",
    "\n",
    "# tools object, which contains one or more function declarations\n",
    "tools = types.Tool(function_declarations=[weather_function])\n",
    "config = types.GenerateContentConfig(tools=[tools])\n",
    "\n",
    "# Step 1: User asks a question\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    # send's user input \n",
    "    contents=[types.Content(role=\"user\", parts=[types.Part(text=\"What's the temperature in Bengaluru?\")])],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "# checking whether the model response contains function call\n",
    "tool_call = response.candidates[0].content.parts[0].function_call\n",
    "\n",
    "# Manual Pattern (When we reconstruct history)\n",
    "# Manually call the function and send response back to the model.\n",
    "if tool_call and tool_call.name == \"get_current_temperature\":\n",
    "    result = get_current_temperature(**tool_call.args)\n",
    "    print(f\"Function execution result: {result}\")\n",
    "    followup_response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    # we are sending second request( the first query, the function call made by the model, the function's result ) \n",
    "    contents=[\n",
    "        types.Content(\n",
    "            role=\"user\",\n",
    "            parts=[types.Part(text=\"What's the temperature in Bengaluru?\")]\n",
    "        ),\n",
    "        types.Content(\n",
    "            role=\"model\",\n",
    "            parts=[types.Part(function_call=tool_call)]\n",
    "        ),\n",
    "        types.Content(\n",
    "            role=\"function\",\n",
    "            parts=[types.Part(\n",
    "                function_response=types.FunctionResponse(\n",
    "                    name=tool_call.name,\n",
    "                    response=result\n",
    "                )\n",
    "            )]\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "    print(\"Model’s natural language reply:\")\n",
    "    print(followup_response.candidates[0].content.parts[0].text)\n",
    "\n",
    "else:\n",
    "    print(\"No function call found in the response.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3988047e-c7ae-4cba-873a-4ab2a6dfce7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function calling with thinking\n",
    "# It tests whether the Gemini model can reason (\"think\") before answering by showing its internal thought process.\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "grounding_tool = types.Tool(\n",
    "    google_search=types.GoogleSearch()\n",
    ")\n",
    "\n",
    "config = types.GenerateContentConfig(\n",
    "    tools=[grounding_tool],\n",
    "    thinking_config=types.ThinkingConfig(\n",
    "        include_thoughts=True  # ask model to return reasoning so we know how the model thinks to pick the function/tools\n",
    "    ),\n",
    ")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"List top 10 restaurants in bengaluru which is available in  today(10/23/2025) \",\n",
    "    config=config,\n",
    "    \n",
    ")\n",
    "\n",
    "#print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c7d9c61-7576-4c9e-a714-1b49591b19b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thoughts:\n",
      "**Gathering Restaurant Insights**\n",
      "\n",
      "I'm currently focused on initial restaurant research. My plan involves searching for \"best restaurants in Bengaluru\" and \"top-rated restaurants Bengaluru\" to build a solid foundation. Next, I will check restaurant availability for the date October 23, 2025. This should give a practical shortlist. I'm aiming for ten restaurants that consistently get high ratings and are confirmed to be open.\n",
      "\n",
      "\n",
      "\n",
      "----\n",
      "Thoughts:\n",
      "**Narrowing Restaurant Choices**\n",
      "\n",
      "I've sifted through numerous \"best of\" lists and articles about new restaurants in Bengaluru, specifically for October 2025, which is quite handy.  My immediate focus is narrowing this down to the top 10, considering their popularity and recent mentions.  While I haven't found direct confirmations of their operating status for today, I'm inferring availability based on the recency of the information.\n",
      "\n",
      "\n",
      "\n",
      "----\n",
      "Final Answer:\n",
      "Here are 10 top-rated restaurants in Bengaluru, with their availability inferred for today, October 23, 2025, based on recent articles and general operational information:\n",
      "\n",
      "1.  **Circa 11, Indiranagar** - This establishment transforms from a café-style spot in the mornings to a dinner destination with cocktails. It's highlighted as a new and notable opening in October 2025, suggesting current operation.\n",
      "2.  **Zarqash, The Ritz-Carlton** - Offering lunch and dinner services, this Cantonese destination with unique decor and dishes like chocolate dim sum is also featured among new restaurants for October 2025.\n",
      "3.  **Marseli Café & Patisserie, HSR Layout** - Known for its European patisserie offerings and popular babka, Marseli is mentioned as a new restaurant to try in October 2025.\n",
      "4.  **Royal China, Vittal Mallya Road** - This iconic Mumbai Chinese eatery recently opened an outpost in Bengaluru, bringing its signature Cantonese classics to the city in October 2025.\n",
      "5.  **Jamming Goat, Marathahalli** - This brew garden, known for its seafood-heavy menu with Goan influences, is another recent opening in Bengaluru as of October 2025.\n",
      "6.  **Mirth, Indiranagar** - Described as a cozy spot bringing back the '90s pub scene with handcrafted whiskey cocktails and Asian flavors blended with Maharashtrian and Goan cuisines, Mirth is a new October 2025 opening.\n",
      "7.  **By The Blue, Grand Mercure Bangalore, Koramangala** - This stunning poolside restaurant at Grand Mercure Bangalore offers a menu celebrating age-old Indian cooking techniques and is listed among new restaurants to check out in October 2025.\n",
      "8.  **Oko, The Lalit Ashok Bangalore** - Known for its exquisite dining experience with Pan-Asian fare and city views, Oko is consistently listed among Bengaluru's top restaurants. It's open until 10:30 PM.\n",
      "9.  **Toscano, UB City** - An Italian restaurant praised for its authentic flavors, upbeat music, and alfresco dining, Toscano is a well-regarded fine dining option in Bengaluru.\n",
      "10. **The Black Pearl, Marathahalli** - India's largest pirate-themed restaurant, it is famous for its unique interiors and mouth-watering barbecue dishes. It's also mentioned with a 5.0 rating in recent customer reviews.\n",
      "\n",
      "Please note that while these restaurants are highly rated and have recent mentions confirming their operation in October 2025, it is always advisable to make a reservation or call ahead to confirm specific operating hours and availability on October 23, 2025.\n"
     ]
    }
   ],
   "source": [
    "for part in response.candidates[0].content.parts:\n",
    "    if getattr(part, \"thought\", False):\n",
    "        print(\"Thoughts:\")\n",
    "        print(part.text)\n",
    "        print(\"----\")\n",
    "    else:\n",
    "        print(\"Final Answer:\")\n",
    "        print(part.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1c80ff6-050c-43f3-a31f-ff0677b75d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspecting Thought Signatures- mostly it will be displayed when we make function calling, to tell the model what operation performed in before request\n",
    "# only when both thinking and function calling are enabled and the model generates thoughts.\n",
    "\n",
    "# checking wheather thought signature is working fine\n",
    "import base64\n",
    "# After receiving a response from a model with thinking enabled\n",
    "# response = client.models.generate_content(...)\n",
    "\n",
    "# The signature is attached to the response part containing the function call\n",
    "\n",
    "\n",
    "for idx, part in enumerate(response.candidates[0].content.parts):\n",
    "    if part.thought_signature:\n",
    "        print(f\"Found thought signature in part {idx}: {part.thought_signature}\")\n",
    "\n",
    "        print(base64.b64encode(part.thought_signature).decode(\"utf-8\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64bc7b03-e456-490b-b1a1-8b54c6410d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n"
     ]
    }
   ],
   "source": [
    "# Function calling with thinking\n",
    "# here, we test how the model reasons and dynamically selects functions to call based on the our request.\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class cost(BaseModel):\n",
    "    dish_cost: int\n",
    "client = genai.Client()\n",
    "\n",
    "\n",
    "def bhai_biryani(dish:str):\n",
    "    \"\"\" return the cost of the dish, where the dish and it's cost is stored as key value pair in menu dictionary\"\"\"\n",
    "    menu = {'chicken fried rice':100, 'biryani':90, 'half kabab':50}\n",
    "    return menu.get(dish,0)\n",
    "def KFC(dish:str):\n",
    "    \"\"\" return the dish cost(key value)\"\"\"\n",
    "    menu={'biryani':200,'fried chicken':600}\n",
    "    return menu.get(dish)\n",
    "\n",
    "bhai_biryani_decl = types.FunctionDeclaration.from_callable(callable=bhai_biryani, client=client)\n",
    "KFC_decl = types.FunctionDeclaration.from_callable(callable=KFC, client=client)\n",
    "\n",
    "func_tools = types.Tool(function_declarations=[bhai_biryani_decl, KFC_decl])\n",
    "\n",
    "\n",
    "config = types.GenerateContentConfig(\n",
    "    tools=[func_tools],\n",
    "    thinking_config=types.ThinkingConfig(\n",
    "        include_thoughts=True  \n",
    "    ),\n",
    "    \n",
    ")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    #contents=\"what is the total cost if i order two biryani from bhai_biryani  \",\n",
    "    contents=\"what is the cost of  biryani in bhai biryani  \",\n",
    "    config=config,\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "for part in response.candidates[0].content.parts:\n",
    "    if part.function_call:\n",
    "        func_name = part.function_call.name\n",
    "        func_args = part.function_call.args\n",
    "        # Call the corresponding Python function manually\n",
    "        if func_name == \"bhai_biryani\":\n",
    "            result_value = bhai_biryani(**func_args)\n",
    "        elif func_name == \"KFC\":\n",
    "            result_value = KFC(**func_args)\n",
    "        \n",
    "        # Wrap in Pydantic model for structured access\n",
    "        result = cost(dish_cost=result_value)\n",
    "        print(result.dish_cost)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b362f90-911d-4822-8a30-6316db0f7044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- MODEL THOUGHTS ---\n",
      "**Thinking Through the Biryani Request**\n",
      "\n",
      "Okay, so someone's asking about the price of biryani at \"bhai biryani.\"  That's straightforward. I've got this `bhai_biryani` function in the `default_api` that's designed for this exact purpose. It's meant to fetch dish prices from the \"bhai biryani\" menu. The key is that the function needs the dish name as an argument.  Therefore, to get the price of their desired biryani, I just need to call `bhai_biryani` and pass \"biryani\" as the `dish` value. Easy peasy.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- MODEL THOUGHTS ---\")\n",
    "for part in response.candidates[0].content.parts:\n",
    "    if getattr(part, \"thought\", False):\n",
    "        print(part.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6092427f-591c-4050-a43b-54601aed7942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rate of beef rice is 40 and curd rice is 50.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# parallel function calling\n",
    "# verify the Gemini model’s parallel function calling capability.\n",
    "\n",
    "\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "def vegeterian(dish:str)->int:\n",
    "    \"\"\" Get the rate of the vegeterian dish\n",
    "\n",
    "    Args:\n",
    "        dish : name of the vegeterian dish, e.g: sambar rice\n",
    "\n",
    "    Returns:\n",
    "        dictionary where the value is rate\n",
    "    \"\"\"\n",
    "    menu ={'curd rice':50,'sambar rice':50, 'parotta':40}\n",
    "    return {\"rate \": menu.get(dish,0)}\n",
    "\n",
    "\n",
    "def non_vegeterian(dish:str)->int:\n",
    "    \"\"\" Get the rate of the non vegeterian dish\n",
    "\n",
    "    Args:\n",
    "        dish : name of the non vegeterian dish, e.g: chicken rice\n",
    "\n",
    "    Returns:\n",
    "        dictionary where the value is rate\n",
    "    \"\"\"\n",
    "    menu ={'chicken rice':50,'mutton rice':50, 'beef rice':40}\n",
    "    return {\"rate \": menu.get(dish,0)}\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "\"\"\"\n",
    "# configure function calling mode, by default AUTO, ANY - prefer functions first\n",
    "tool_config = types.ToolConfig(\n",
    "    function_calling_config = types.FunctionCallingConfig(\n",
    "        mode = 'ANY', allowed_function_names = [\"vegeterian\",\"non_vegeterian\"]\n",
    "    )\n",
    ")\n",
    "\"\"\"\n",
    "config = types.GenerateContentConfig(\n",
    "\n",
    "    #passing directly the function itself\n",
    "    tools = [vegeterian,non_vegeterian],\n",
    "    #tool_config = tool_config,\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model = \"gemini-2.5-flash\",\n",
    "    contents = \"what is the rate of beef rice and curd rice\",\n",
    "    config = config,\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b461231b-bc64-43ec-a5e8-4af5e4ca9846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponse(\n",
       "  automatic_function_calling_history=[\n",
       "    UserContent(\n",
       "      parts=[\n",
       "        Part(\n",
       "          text='what is the rate of beef rice and curd rice'\n",
       "        ),\n",
       "      ],\n",
       "      role='user'\n",
       "    ),\n",
       "    Content(\n",
       "      parts=[\n",
       "        Part(\n",
       "          function_call=FunctionCall(\n",
       "            args={\n",
       "              <... Max depth ...>: <... Max depth ...>\n",
       "            },\n",
       "            name='non_vegeterian'\n",
       "          ),\n",
       "          thought_signature=b'\\n\\xf6\\x03\\x01\\xd1\\xed\\x8ao\\x10\\xbb\\x10W\\xcf\\xab\\x18\\t\\xf3d\\x93\\x1b\"%\\x1d\\xf0\\x10[i\\x0bz\\x9c\\x95\\x1cJ\\xa3\\x81\\x9a#\\xc5\\xc5\\xe0\\x1c\\x0c+p\\x06\\xa8\\x18\\xe2\\xa7Cw lh\\t\\xcc8\\xf0\\xbe\\xd3\\xf5/\\xc4\\x81\\xf96\\xe5\\xa0qy\\xfa\\x80t@@\\xc2\\x9d\\xcf\\x0b_F\\x94\\xd0n\\\\Nw\\xd8\\xfd\\xbeW\\xb530\\xf4\\xd5\\x04...'\n",
       "        ),\n",
       "        Part(\n",
       "          function_call=FunctionCall(\n",
       "            args={\n",
       "              <... Max depth ...>: <... Max depth ...>\n",
       "            },\n",
       "            name='vegeterian'\n",
       "          )\n",
       "        ),\n",
       "      ],\n",
       "      role='model'\n",
       "    ),\n",
       "    Content(\n",
       "      parts=[\n",
       "        Part(\n",
       "          function_response=FunctionResponse(\n",
       "            name='non_vegeterian',\n",
       "            response={\n",
       "              <... Max depth ...>: <... Max depth ...>\n",
       "            }\n",
       "          )\n",
       "        ),\n",
       "        Part(\n",
       "          function_response=FunctionResponse(\n",
       "            name='vegeterian',\n",
       "            response={\n",
       "              <... Max depth ...>: <... Max depth ...>\n",
       "            }\n",
       "          )\n",
       "        ),\n",
       "      ],\n",
       "      role='user'\n",
       "    ),\n",
       "  ],\n",
       "  candidates=[\n",
       "    Candidate(\n",
       "      content=Content(\n",
       "        parts=[\n",
       "          Part(\n",
       "            text='The rate of beef rice is 40 and curd rice is 50.'\n",
       "          ),\n",
       "        ],\n",
       "        role='model'\n",
       "      ),\n",
       "      finish_reason=<FinishReason.STOP: 'STOP'>,\n",
       "      index=0\n",
       "    ),\n",
       "  ],\n",
       "  model_version='gemini-2.5-flash',\n",
       "  response_id='dv7jaMenGeuAvr0PtLep0Ag',\n",
       "  sdk_http_response=HttpResponse(\n",
       "    headers=<dict len=11>\n",
       "  ),\n",
       "  usage_metadata=GenerateContentResponseUsageMetadata(\n",
       "    candidates_token_count=17,\n",
       "    prompt_token_count=229,\n",
       "    prompt_tokens_details=[\n",
       "      ModalityTokenCount(\n",
       "        modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "        token_count=229\n",
       "      ),\n",
       "    ],\n",
       "    total_token_count=246\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fe6af61a-db5c-4f65-b84b-8949dddafe9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['thought_signature', 'function_call', 'function_call'], returning concatenated parsed result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rate of beef rice is 40 and curd rice is 50.\n",
      "beef rice: 40\n",
      "curd rice: 50\n"
     ]
    }
   ],
   "source": [
    "# parallel function calling - how can i add structured output(pydantic) in this code:\n",
    "#here we are testing how Gemini handles parallel function calls and structured outputs together,\n",
    "\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class dishRate(BaseModel):\n",
    "    dish:str\n",
    "    rate:int\n",
    "\n",
    "def vegeterian(dish:str)->int:\n",
    "    \"\"\" Get the rate of the vegeterian dish\n",
    "\n",
    "    Args:\n",
    "        dish : name of the vegeterian dish, e.g: sambar rice\n",
    "\n",
    "    Returns:\n",
    "        dishRate object which contains dish name and dish rate\n",
    "    \"\"\"\n",
    "    menu ={'curd rice':50,'sambar rice':50, 'parotta':40}\n",
    "    return dishRate(dish=dish, rate = menu.get(dish,0))\n",
    "\n",
    "\n",
    "def non_vegeterian(dish:str)->int:\n",
    "    \"\"\" Get the rate of the non vegeterian dish\n",
    "\n",
    "    Args:\n",
    "        dish : name of the non vegeterian dish, e.g: chicken rice\n",
    "\n",
    "    Returns:\n",
    "        dishRate object which contains dish name and dish rate\n",
    "    \"\"\"\n",
    "    menu ={'chicken rice':50,'mutton rice':50, 'beef rice':40}\n",
    "    return dishRate(dish=dish, rate = menu.get(dish,0))\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "\"\"\"\n",
    "# configure function calling mode, by default AUTO, ANY - prefer functions first\n",
    "tool_config = types.ToolConfig(\n",
    "    function_calling_config = types.FunctionCallingConfig(\n",
    "        mode = 'ANY', allowed_function_names = [\"vegeterian\",\"non_vegeterian\"]\n",
    "    )\n",
    ")\n",
    "\"\"\"\n",
    "# for strutured output the function declaration is optimal(if i provide here, automatic function calling is not take place)\n",
    "veg_decl = types.FunctionDeclaration.from_callable(callable = vegeterian, client=client)\n",
    "nonveg_decl = types.FunctionDeclaration.from_callable(callable = non_vegeterian,client=client)\n",
    "tools = types.Tool(function_declarations=[veg_decl,nonveg_decl])\n",
    "\n",
    "config = types.GenerateContentConfig(\n",
    "    \n",
    "    #tools = [tools],\n",
    "    tools = [vegeterian,non_vegeterian],\n",
    "    automatic_function_calling=types.AutomaticFunctionCallingConfig(),\n",
    "    thinking_config=types.ThinkingConfig(include_thoughts=True),\n",
    "    response_schema = list[dishRate],\n",
    "    #response_mime_type = \"application/json\",\n",
    "    \n",
    "    #tool_config = tool_config,\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model = \"gemini-2.5-flash\",\n",
    "    contents = \"what is the rate of beef rice and curd rice\",\n",
    "    config = config,\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "print(response)\n",
    "print(response.text)\n",
    "structured_result: list[dishRate] = response.parsed\n",
    "for dish in structured_result:\n",
    "    print(f\"{dish.dish}: {dish.rate}\")\n",
    "\"\"\"\n",
    "print(response.text)\n",
    "results = []\n",
    "\n",
    "# iterate over all automatic_function_calling_history items\n",
    "for history_item in response.automatic_function_calling_history:\n",
    "    if hasattr(history_item, \"parts\"):\n",
    "        for part in history_item.parts:\n",
    "            if part.function_response:\n",
    "                func_result = part.function_response.response.get(\"result\")\n",
    "                if func_result:\n",
    "                    results.append(func_result)\n",
    "\n",
    "# results now contains DishRate objects\n",
    "for dish in results:\n",
    "    print(f\"{dish.dish}: {dish.rate}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eab107b-532d-4a95-bd88-497ca718ea57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f444735a-8eed-45c1-b6e5-d0bdd39c9c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['thought_signature', 'function_call', 'function_call'], returning concatenated parsed result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
      "Warning: there are non-text parts in the response: ['thought_signature'], returning concatenated parsed result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
      "Warning: there are non-text parts in the response: ['thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model did not return a structured response.\n",
      "The rate of beef rice is 40 and curd rice is 50.\n"
     ]
    }
   ],
   "source": [
    "# parallel function calling \n",
    "# here we test concurrent async workflow with schema-validated results.\n",
    "\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from pydantic import BaseModel\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "class dishRate(BaseModel):\n",
    "    dish:str\n",
    "    rate:int\n",
    "class allRates(BaseModel):\n",
    "    dishes:list[dishRate]\n",
    "\n",
    "def vegeterian(dish:str)->dishRate:\n",
    "    \"\"\" Get the rate of the vegeterian dish\n",
    "    \"\"\"\n",
    "    menu ={'curd rice':50,'sambar rice':50, 'parotta':40}\n",
    "    return dishRate(dish=dish, rate = menu.get(dish,0))\n",
    "\n",
    "\n",
    "def non_vegeterian(dish:str)->dishRate:\n",
    "    \"\"\" Get the rate of the non vegeterian dish\n",
    "    \"\"\"\n",
    "    menu ={'chicken rice':50,'mutton rice':50, 'beef rice':40}\n",
    "    return dishRate(dish=dish, rate = menu.get(dish,0))\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# configure function calling mode, by default AUTO, ANY - prefer functions first\n",
    "tool_config = types.ToolConfig(\n",
    "    function_calling_config = types.FunctionCallingConfig(mode = 'ANY')\n",
    ")\n",
    "\n",
    "# for strutured output the function declaration is optimal(if i provide here, automatic function calling is not take place)\n",
    "#veg_decl = types.FunctionDeclaration.from_callable(callable = vegeterian, client=client)\n",
    "#nonveg_decl = types.FunctionDeclaration.from_callable(callable = non_vegeterian,client=client)\n",
    "#tool = types.Tool(function_declarations=[veg_decl,nonveg_decl])\n",
    "async def main():\n",
    "    config = types.GenerateContentConfig(\n",
    "        \n",
    "        #tools = [tool],\n",
    "        tools = [vegeterian,non_vegeterian],\n",
    "        automatic_function_calling=types.AutomaticFunctionCallingConfig( ),\n",
    "        thinking_config=types.ThinkingConfig(include_thoughts=True),\n",
    "        #response_schema = allRates,\n",
    "        #response_mime_type = \"application/json\",\n",
    "        response_schema=allRates,\n",
    "        tool_config = types.ToolConfig(\n",
    "            function_calling_config=types.FunctionCallingConfig(mode=\"AUTO\")\n",
    "        ),\n",
    "        \n",
    "    )\n",
    "\n",
    "    response = await client.aio.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        #contents=\"what is the rate of beef rice(non veg) and curd rice(veg)\",\n",
    "        contents =\"what is the rate of beef rice(non veg) and curd rice(veg)\" ,\n",
    "\n",
    "        config=config,\n",
    "    )\n",
    "    structured_result: allRates = response.parsed\n",
    "    if response.parsed:\n",
    "        for dish in response.parsed.dishes:\n",
    "            print(f\"{dish.dish}: {dish.rate}\")\n",
    "    else:\n",
    "        print(\"Model did not return a structured response.\")\n",
    "        print(response.text)\n",
    "    \n",
    "\n",
    "\n",
    "await main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dc6ce23b-ea7d-45a3-89ed-6117f14b6577",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The price for chicken pizza is 100 and the GST for it is 10.\n"
     ]
    }
   ],
   "source": [
    "# compositional function calling\n",
    "# This code tests Gemini's compositional function calling — where functions are called sequentially, each depending on prior results. \n",
    "\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import json\n",
    "def pizza_bakery_mani(flavour:str)->dict:\n",
    "    \"\"\" Get the price for a given given flavour\"\"\"\n",
    "    menu = {'chicken':100,'tomato':70,'corn':50,'beef':80}\n",
    "    return {'flavour':flavour,'price':menu.get(flavour)}\n",
    "\n",
    "def pizza_bakery_mani_gst(price:int)->dict:\n",
    "    \"\"\" return the gst for this price\"\"\"\n",
    "    return {\"gst_price\":price*0.10}\n",
    "\n",
    "client = genai.Client()\n",
    "config = types.GenerateContentConfig(\n",
    "    tools=[pizza_bakery_mani, pizza_bakery_mani_gst ]\n",
    ")\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"Get me the price and gst for chicken(flavour) pizza from pizza bakery mani\",\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e12bbf2-bcde-47b8-8062-a42a2465b4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['thought_signature', 'function_call', 'function_call'], returning concatenated parsed result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
      "Warning: there are non-text parts in the response: ['thought_signature', 'function_call', 'function_call'], returning concatenated parsed result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
      "Warning: there are non-text parts in the response: ['thought_signature', 'function_call', 'function_call'], returning concatenated parsed result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
      "Warning: there are non-text parts in the response: ['thought_signature', 'function_call', 'function_call'], returning concatenated parsed result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
      "Warning: there are non-text parts in the response: ['thought_signature', 'function_call', 'function_call'], returning concatenated parsed result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
      "Warning: there are non-text parts in the response: ['thought_signature', 'function_call', 'function_call'], returning concatenated parsed result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
      "Warning: there are non-text parts in the response: ['thought_signature', 'function_call', 'function_call'], returning concatenated parsed result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
      "Warning: there are non-text parts in the response: ['thought_signature', 'function_call', 'function_call'], returning concatenated parsed result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
      "Warning: there are non-text parts in the response: ['thought_signature', 'function_call', 'function_call'], returning concatenated parsed result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
      "Warning: there are non-text parts in the response: ['thought_signature', 'function_call', 'function_call'], returning concatenated parsed result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beef rice: 40\n",
      "curd rice: 50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nprint(response.text)\\nresults = []\\n\\n# iterate over all automatic_function_calling_history items\\nfor history_item in response.automatic_function_calling_history:\\n    if hasattr(history_item, \"parts\"):\\n        for part in history_item.parts:\\n            if part.function_response:\\n                func_result = part.function_response.response.get(\"result\")\\n                if func_result:\\n                    results.append(func_result)\\n\\n# results now contains DishRate objects\\nfor dish in results:\\n    print(f\"{dish.dish}: {dish.rate}\")\\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parallel function calling \n",
    "\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from pydantic import BaseModel\n",
    "import asyncio\n",
    "\n",
    "\n",
    "class dishRate(BaseModel):\n",
    "    dish:str\n",
    "    rate:int\n",
    "class allRates(BaseModel):\n",
    "    dishes:list[dishRate]\n",
    "\n",
    "def vegeterian(dish:str)->int:\n",
    "    \"\"\" Get the rate of the vegeterian dish\n",
    "    \"\"\"\n",
    "    menu ={'curd rice':50,'sambar rice':50, 'parotta':40}\n",
    "    return dishRate(dish=dish, rate = menu.get(dish,0))\n",
    "\n",
    "\n",
    "def non_vegeterian(dish:str)->int:\n",
    "    \"\"\" Get the rate of the non vegeterian dish\n",
    "    \"\"\"\n",
    "    menu ={'chicken rice':50,'mutton rice':50, 'beef rice':40}\n",
    "    return dishRate(dish=dish, rate = menu.get(dish,0))\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# configure function calling mode, by default AUTO, ANY - prefer functions first\n",
    "tool_config = types.ToolConfig(\n",
    "    function_calling_config = types.FunctionCallingConfig(mode = 'ANY')\n",
    ")\n",
    "\n",
    "# for strutured output the function declaration is optimal(if i provide here, automatic function calling is not take place)\n",
    "#veg_decl = types.FunctionDeclaration.from_callable(callable = vegeterian, client=client)\n",
    "#nonveg_decl = types.FunctionDeclaration.from_callable(callable = non_vegeterian,client=client)\n",
    "#tool = types.Tool(function_declarations=[veg_decl,nonveg_decl])\n",
    "\n",
    "config = types.GenerateContentConfig(\n",
    "    \n",
    "    #tools = [tool],\n",
    "    tools = [vegeterian,non_vegeterian],\n",
    "    automatic_function_calling=types.AutomaticFunctionCallingConfig(),\n",
    "    thinking_config=types.ThinkingConfig(include_thoughts=True),\n",
    "    response_schema = allRates,\n",
    "    #response_mime_type = \"application/json\",\n",
    "    \n",
    "    tool_config = types.ToolConfig(\n",
    "        function_calling_config=types.FunctionCallingConfig(mode=\"ANY\")\n",
    "    ),\n",
    "    \n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "response = client.models.generate_content(\n",
    "    model = \"gemini-2.5-flash\",\n",
    "    contents = \"what is the rate of beef rice(non veg) and curd rice(veg)\",\n",
    "    config = config,\n",
    ")\n",
    "\"\"\"\n",
    "async def main():\n",
    "    response = await client.aio.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=\"what is the rate of beef rice(non veg) and curd rice(veg)\",\n",
    "        config=config,\n",
    "    )\n",
    "    results = []\n",
    "    if response.automatic_function_calling_history:\n",
    "        # Consider only the last history item (most recent function call response)\n",
    "        latest_history = response.automatic_function_calling_history[-1]\n",
    "        if hasattr(latest_history, \"parts\"):\n",
    "            for part in latest_history.parts:\n",
    "                if part.function_response:\n",
    "                    func_result = part.function_response.response.get(\"result\")\n",
    "                    if func_result:\n",
    "                        results.append(func_result)\n",
    "    \n",
    "    # Now print results without duplication\n",
    "    seen = set()\n",
    "    for dish in results:\n",
    "        key = (dish.dish, dish.rate)\n",
    "        if key not in seen:\n",
    "            print(f\"{dish.dish}: {dish.rate}\")\n",
    "            seen.add(key)\n",
    "    \"\"\"\n",
    "    print(\"Automatic Function Calls:\\n\", response.automatic_function_calling_history)\n",
    "    \n",
    "    if response.parsed:\n",
    "        structured_result: allRates = response.parsed\n",
    "        for dish in structured_result.dishes:\n",
    "            print(f\"{dish.dish}: {dish.rate}\")\n",
    "    else:\n",
    "        print(\"Structured parse failed, using raw extraction.\")\n",
    "        results = []\n",
    "        for history_item in response.automatic_function_calling_history:\n",
    "            if hasattr(history_item, \"parts\"):\n",
    "                for part in history_item.parts:\n",
    "                    if part.function_response:\n",
    "                        func_result = part.function_response.response.get(\"result\")\n",
    "                        if func_result:\n",
    "                            results.append(func_result)\n",
    "        for item in results:\n",
    "            print(f\"{item.dish}: {item.rate}\")\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "#asyncio.run(main())\n",
    "await main()\n",
    "\"\"\"\n",
    "print(response.text)\n",
    "results = []\n",
    "\n",
    "# iterate over all automatic_function_calling_history items\n",
    "for history_item in response.automatic_function_calling_history:\n",
    "    if hasattr(history_item, \"parts\"):\n",
    "        for part in history_item.parts:\n",
    "            if part.function_response:\n",
    "                func_result = part.function_response.response.get(\"result\")\n",
    "                if func_result:\n",
    "                    results.append(func_result)\n",
    "\n",
    "# results now contains DishRate objects\n",
    "for dish in results:\n",
    "    print(f\"{dish.dish}: {dish.rate}\")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33a7b68e-44b6-46cf-9fe9-eb2ad633d1ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['thought_signature', 'function_call'], returning concatenated parsed result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool Call: get_weather_forecast(location=chennai)\n",
      "Tool Response: {'temperature': 25, 'unit': 'celsius'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['thought_signature', 'function_call'], returning concatenated parsed result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool Call: set_thermostat_temperature(temperature=25)\n",
      "Tool Response: {'status': 'success'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['thought_signature'], returning concatenated parsed result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
      "Warning: there are non-text parts in the response: ['thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model did not return structured output.\n",
      "The weather forecast for Chennai is 25 degrees Celsius. I have set the thermostat temperature to 25 degrees Celsius.\n"
     ]
    }
   ],
   "source": [
    "# here we testing structured compositional function-calling test\n",
    "\n",
    "import os\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# -----------------------------\n",
    "# Define Pydantic schema\n",
    "# -----------------------------\n",
    "class ThermostatResult(BaseModel):\n",
    "    location: str\n",
    "    temperature: int\n",
    "    #thermostat_setting: int\n",
    "    status: str\n",
    "\n",
    "#--------------------------------\n",
    "# Define functions\n",
    "# -----------------------------\n",
    "def get_weather_forecast(location: str) -> dict:\n",
    "    \"\"\"Gets the current weather temperature for a given location.\"\"\"\n",
    "\n",
    "    print(f\"Tool Call: get_weather_forecast(location={location})\")\n",
    "    print(\"Tool Response: {'temperature': 25, 'unit': 'celsius'}\")\n",
    "    return {\"temperature\": 25, \"unit\": \"celsius\"}  \n",
    "\n",
    "def set_thermostat_temperature(temperature: int) -> dict:\n",
    "    \"\"\"Sets the thermostat to a desired temperature.\"\"\"\n",
    "    print(f\"Tool Call: set_thermostat_temperature(temperature={temperature})\")\n",
    "    print(\"Tool Response: {'status': 'success'}\")\n",
    "    return {\"status\": \"success\"}\n",
    "\n",
    "# -----------------------------\n",
    "# 3️⃣ Configure client + schema\n",
    "# -----------------------------\n",
    "client = genai.Client()\n",
    "\n",
    "config = types.GenerateContentConfig(\n",
    "    tools=[get_weather_forecast, set_thermostat_temperature],\n",
    "    automatic_function_calling=types.AutomaticFunctionCallingConfig(),\n",
    "    response_schema=ThermostatResult,   \n",
    "    thinking_config=types.ThinkingConfig(include_thoughts=False),\n",
    "    \n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 4️⃣ Run model\n",
    "# -----------------------------\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    #contents=\"If it's warmer than 20°C in London, set the thermostat to 20°C, otherwise set it to 18°C.\",\n",
    "    #contents = \"\"\"\n",
    "    #        If it's warmer than 20°C in London, set thermostat to 20°C, otherwise 18°C.\n",
    "    #        Respond only in valid JSON matching  schema\n",
    "    #        \"\"\",\n",
    "    contents = \" Get weather forecast for chennai and  set thermostat temperature from getting value from weather forecast for chennai\",\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 5️⃣ Access structured output\n",
    "# -----------------------------\n",
    "if response.parsed:\n",
    "    result: ThermostatResult = response.parsed\n",
    "    print(\"✅ Structured Result:\")\n",
    "    print(result.json(indent=2))\n",
    "\n",
    "    # You can reuse later like:\n",
    "    print(\"\\nLater use example:\")\n",
    "    print(f\"Thermostat was set to {result.thermostat_setting}°C in {result.location}\")\n",
    "else:\n",
    "    print(\"Model did not return structured output.\")\n",
    "    print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bcad5ff9-071c-4b84-993d-ee947c6d3efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_weather_forecast {'result': {'temperature': 25, 'unit': 'celsius'}}\n",
      "set_thermostat_temperature {'result': {'status': 'success'}}\n"
     ]
    }
   ],
   "source": [
    "collected_data = {\n",
    "    \"location\": \"Chennai\",\n",
    "    \"temperature\": None,\n",
    "    \"status\": None,\n",
    "}\n",
    "\n",
    "for content in response.automatic_function_calling_history:\n",
    "    for part in content.parts:\n",
    "        if hasattr(part, \"function_response\") and part.function_response:\n",
    "            func_name = part.function_response.name\n",
    "            func_resp = part.function_response.response or {}\n",
    "            print(func_name, func_resp)\n",
    "            if func_name == \"get_weather_forecast\":\n",
    "                collected_data[\"temperature\"] = func_resp['result'].get(\"temperature\")\n",
    "            elif func_name == \"set_thermostat_temperature\":\n",
    "                collected_data[\"status\"] = func_resp['result'].get(\"status\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4fcf89ec-a5b8-45d9-a4d8-24ba8938a663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'location': 'Chennai', 'temperature': 25, 'status': 'success'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collected_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a6bc6c05-1708-42e0-a66e-7a66b33971d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"location\": \"Chennai\",\n",
      "  \"temperature\": 25,\n",
      "  \"status\": \"success\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "json_str = result.model_dump_json(indent=2)\n",
    "print(json_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "191474f3-409b-42b5-95bf-39a0eb573754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'location': 'Chennai', 'temperature': 25, 'status': 'success'}\n"
     ]
    }
   ],
   "source": [
    "dict_data = result.model_dump()\n",
    "print(dict_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7a401143-7d85-4d50-a219-5f25c1a9ba51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location: Chennai\n",
      "Temperature: 25\n",
      "Status: success\n"
     ]
    }
   ],
   "source": [
    "print(f\"Location: {result.location}\")\n",
    "print(f\"Temperature: {result.temperature}\")\n",
    "print(f\"Status: {result.status}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b8e53758-ac6e-48fb-9196-ccd70a892b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['thought_signature', 'function_call'], returning concatenated parsed result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tool Call] get_weather_forecast(London)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['thought_signature', 'function_call'], returning concatenated parsed result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tool Call] set_thermostat_temperature(20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['thought_signature'], returning concatenated parsed result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
      "Warning: there are non-text parts in the response: ['thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Structured parse failed. Attempting manual parsing...\n",
      "\n",
      " No valid structured data found. Model text output:\n",
      "\n",
      "The thermostat has been set to 20°C.\n"
     ]
    }
   ],
   "source": [
    "# this is code is same as previous of compositional with structured schema which includes this new code contains, validation error \n",
    "#with our schema and model reasoning for choosing the function\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from pydantic import BaseModel, ValidationError\n",
    "\n",
    "\n",
    "class ThermostatResult(BaseModel):\n",
    "    location: str\n",
    "    current_temperature: int\n",
    "    status: str\n",
    "\n",
    "\n",
    "\n",
    "def get_weather_forecast(location: str) -> dict:\n",
    "    \"\"\"Simulate weather data retrieval\"\"\"\n",
    "    print(f\"[Tool Call] get_weather_forecast({location})\")\n",
    "    return {\"temperature\": 25, \"unit\": \"celsius\"} \n",
    "\n",
    "\n",
    "def set_thermostat_temperature(temperature: int) -> dict:\n",
    "    \"\"\"Simulate thermostat configuration\"\"\"\n",
    "    print(f\"[Tool Call] set_thermostat_temperature({temperature})\")\n",
    "    return {\"status\": \"success\"}\n",
    "\n",
    "\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "config = types.GenerateContentConfig(\n",
    "    tools=[get_weather_forecast, set_thermostat_temperature],\n",
    "    automatic_function_calling=types.AutomaticFunctionCallingConfig(),\n",
    "    thinking_config=types.ThinkingConfig(include_thoughts=True),  \n",
    "    response_schema=ThermostatResult,  \n",
    ")\n",
    "\n",
    "contents = (\n",
    "    \"If it's warmer than 20°C in London, set the thermostat to 20°C. \"\n",
    "    \"Otherwise, set it to 18°C. Return the output in the required structure.\"\n",
    ")\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=contents,\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "\n",
    "if response.parsed:\n",
    "    #  Automatic structured parsing success\n",
    "    result: ThermostatResult = response.parsed\n",
    "    print(\"\\n Structured Output from Gemini:\")\n",
    "    print(result.json(indent=2))\n",
    "\n",
    "else:\n",
    "    # Automatic parse failed — perform manual recovery\n",
    "    print(\"\\n Structured parse failed. Attempting manual parsing...\\n\")\n",
    "\n",
    "    # Extract relevant function call responses\n",
    "    results = []\n",
    "    for candidate in response.candidates:\n",
    "        for part in candidate.content.parts:\n",
    "            if hasattr(part, \"function_response\") and part.function_response:\n",
    "                print(\"name \", part.function_response.name)\n",
    "                response_data = part.function_response.response.get(\"result\")\n",
    "                if response_data:\n",
    "                    try:\n",
    "                        validated = ThermostatResult(**response_data)\n",
    "                        results.append(validated)\n",
    "                    except ValidationError as e:\n",
    "                        print(f\"Schema Validation Error:\\n{e}\")\n",
    "\n",
    "    if results:\n",
    "        print(\" Recovered Structured Output:\")\n",
    "        for r in results:\n",
    "            print(r.json(indent=2))\n",
    "    else:\n",
    "        print(\" No valid structured data found. Model text output:\\n\")\n",
    "        print(response.text)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "639d60ab-7cc8-4bed-bdb5-b5f725468f5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponse(\n",
       "  automatic_function_calling_history=[\n",
       "    UserContent(\n",
       "      parts=[\n",
       "        Part(\n",
       "          text=\"If it's warmer than 20°C in London, set the thermostat to 20°C. Otherwise, set it to 18°C. Return the output in the required structure.\"\n",
       "        ),\n",
       "      ],\n",
       "      role='user'\n",
       "    ),\n",
       "    Content(\n",
       "      parts=[\n",
       "        Part(\n",
       "          text=\"\"\"**Controlling the Thermostat Based on London's Weather**\n",
       "\n",
       "Okay, so the goal is to intelligently manage this thermostat in response to the weather in London.  My thinking is, first things first: I *need* the weather forecast for London.  I'll call the `get_weather_forecast` function, specifying \"London\" as the location.  That'll give me the data I need.\n",
       "\n",
       "Next, I'll extract the temperature reading from that weather data. Now comes the decision-making part.  I'm going to compare that temperature to 20°C. If it's warmer than 20°C, I want to set the thermostat to 20°C; if it's colder or exactly 20°C, then 18°C. Simple.\n",
       "\n",
       "Finally, the output – I should return the result of that `set_thermostat_temperature` function call.  That's how I'll know the thermostat has been adjusted as intended.  It's a straightforward process, but it hinges on accurately fetching and interpreting that London weather data.\n",
       "\"\"\",\n",
       "          thought=True\n",
       "        ),\n",
       "        Part(\n",
       "          function_call=FunctionCall(\n",
       "            args={\n",
       "              <... Max depth ...>: <... Max depth ...>\n",
       "            },\n",
       "            name='get_weather_forecast'\n",
       "          ),\n",
       "          thought_signature=b\"\\n\\xff\\x04\\x01\\xd1\\xed\\x8ao\\x93I\\xec\\xc4Q\\x1d-i\\x96l\\xf7C\\x95\\xab\\xe2\\xb7\\xd5{\\x8b\\x11\\xf7~\\xc3\\x8ao2/\\xcb\\xbc\\x10O\\x96\\x98\\xfb\\xd1EW\\n\\xfd\\x9e\\xc4\\x8f\\xa8\\x0e\\xcf\\x97\\x98\\xa29~l\\xd4\\xdb\\xf7~x\\xed7\\xa8\\x08\\x86*\\x83Jp\\x17\\x9eV\\xbbN\\x9a\\xa9\\xb5\\xe4\\x80'\\xbdO\\x8e'F.\\xa6\\x80O\\xddXy\\x0f...'\n",
       "        ),\n",
       "      ],\n",
       "      role='model'\n",
       "    ),\n",
       "    Content(\n",
       "      parts=[\n",
       "        Part(\n",
       "          function_response=FunctionResponse(\n",
       "            name='get_weather_forecast',\n",
       "            response={\n",
       "              <... Max depth ...>: <... Max depth ...>\n",
       "            }\n",
       "          )\n",
       "        ),\n",
       "      ],\n",
       "      role='user'\n",
       "    ),\n",
       "    Content(\n",
       "      parts=[\n",
       "        Part(\n",
       "          text=\"\"\"**Thermostat Adjustment: A Logical Approach**\n",
       "\n",
       "Okay, so the current temperature in London is a balmy 25°C.  That's quite a bit warmer than the target I've got in mind, which is 20°C.  It's pretty straightforward, really.  Since 25 is greater than 20, I need to adjust the thermostat downwards.  The simplest way to achieve that is to invoke the `set_thermostat_temperature` function and provide it with the desired temperature, which, in this case, is 20.  Let's get that done.\n",
       "\"\"\",\n",
       "          thought=True\n",
       "        ),\n",
       "        Part(\n",
       "          function_call=FunctionCall(\n",
       "            args={\n",
       "              <... Max depth ...>: <... Max depth ...>\n",
       "            },\n",
       "            name='set_thermostat_temperature'\n",
       "          ),\n",
       "          thought_signature=b\"\\n\\xcd\\x01\\x01\\xd1\\xed\\x8ao\\xa0\\xf9\\x18\\x85\\xcb2\\x98<\\xcczt\\xfc\\xbf\\xc8^\\xf9\\xd2\\xa4\\xbd\\xe1\\x808\\x9a\\xcd\\x84\\xfa[\\x7f\\xb4\\x0bif\\x85\\xf6\\xb7\\xf9\\xbcp\\xe4\\xef\\xe4a\\x1c\\xe3\\xf7\\x1a\\xa6\\x9ew\\x0e\\x80_\\x7f5\\x88\\xcd\\xde1\\x9d\\x0e\\x88\\xcb\\xc2\\x01n\\x94\\xe4S$2\\x11\\x07\\xa4T\\xdb\\x9e\\x0f\\xdb'\\xbf\\x01j\\xf9\\x8a\\xd5\\xce\\x93\\x07F...'\n",
       "        ),\n",
       "      ],\n",
       "      role='model'\n",
       "    ),\n",
       "    Content(\n",
       "      parts=[\n",
       "        Part(\n",
       "          function_response=FunctionResponse(\n",
       "            name='set_thermostat_temperature',\n",
       "            response={\n",
       "              <... Max depth ...>: <... Max depth ...>\n",
       "            }\n",
       "          )\n",
       "        ),\n",
       "      ],\n",
       "      role='user'\n",
       "    ),\n",
       "  ],\n",
       "  candidates=[\n",
       "    Candidate(\n",
       "      content=Content(\n",
       "        parts=[\n",
       "          Part(\n",
       "            text=\"\"\"**My Response to the Thermostat Setting**\n",
       "\n",
       "Okay, so I just received confirmation that the thermostat is now set to 20 degrees Celsius. That's good, that's what I intended. Now I need to package this information up according to the expected format. I see the output: `{\"set_thermostat_temperature_response\": {\"result\": {\"status\": \"success\"}}}`. This structure is precisely what I needed – it confirms the successful completion of the action and is represented in a clean, JSON format. This response can now be utilized for further evaluation and action by the system.\n",
       "\"\"\",\n",
       "            thought=True\n",
       "          ),\n",
       "          Part(\n",
       "            text='The thermostat has been set to 20°C.',\n",
       "            thought_signature=b'\\n\\x93\\x02\\x01\\xd1\\xed\\x8ao\\xb4b\\xc41\\xcd^8.~6L\\xfd\\xebe\\xa4p[\\x11\\xdd\\xd1\\xfc\\xe7\\xbb\\x19\\x96\\xad\\x93ee\\xe1\\xa0g2\\x19\\xc3\\xd4\\xc2\\xaa\\x03F\\n\\xbd\\x9b\\x17\\x0fAv\\x87\\xf0w\\x11\\xab\\xfd\\xf5\\x07+\\xd8\\xf7d\\x01T\\xe72\\\\nxB\\xc1\\xf2\\xbd\\x04\\x8b\\xfd\\x1dC\\xc5\\x86\\xf5\\x06\\xfc\\x1a\\xa8\\x8fm-\"\\n\\xbd\\xf2...'\n",
       "          ),\n",
       "        ],\n",
       "        role='model'\n",
       "      ),\n",
       "      finish_reason=<FinishReason.STOP: 'STOP'>,\n",
       "      index=0\n",
       "    ),\n",
       "  ],\n",
       "  model_version='gemini-2.5-flash',\n",
       "  response_id='6Dj9aJO8McGUvr0Prp7iiQs',\n",
       "  sdk_http_response=HttpResponse(\n",
       "    headers=<dict len=11>\n",
       "  ),\n",
       "  usage_metadata=GenerateContentResponseUsageMetadata(\n",
       "    candidates_token_count=12,\n",
       "    prompt_token_count=575,\n",
       "    prompt_tokens_details=[\n",
       "      ModalityTokenCount(\n",
       "        modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "        token_count=575\n",
       "      ),\n",
       "    ],\n",
       "    thoughts_token_count=59,\n",
       "    total_token_count=646\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4bebaf3c-9385-4ef7-9999-45af87bc296d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found thought signature in part 1: b'\\n\\xa7\\x02\\x01\\xd1\\xed\\x8ao\\x83\\x12\\x97\\xea+\\xfc5,\\xe1:\\xbd\\xd2T-\\x04Ux\\'\\x8cLbs\\x96\\xff\\xcaT\\x03\\\\\\x9f\\xd7\\x93=\\nTM\\x93\\x01OgNS\\\\\\x0e\\xa7\\xd1\\x02c\\xea\\x1e\\xd9\\x06\\xac\\xb0~\\x93\\xae\\xe2^\\x85L=\\xb0\\xcd\\x9a\\x08x\\x91\\xd7\\xa4F\\xa6e\\xf2\\xe2\\xc4\\x02\\xa4\\xf4\\xf0\\x13P\\x8e\\xbd\\x88TL\\xdef\\xa9\\x8aW&\\xbb\\x15\\xad\\x87g\\xb1\\x10\\x87\\xacM\\xe1\\xaed2T\\x84\\x98.\\x16\\xa5\\xc8\\xd5\\x0bQ\\x91\\xa0;\\xb3\\xa0\\x06\\xdc\\x08\\xec\\xb7\\x98\\xde\\xf9\\xd2\\x00#\\xb7\\xe6\\x82\\xfb\"\\xbej\\x9dyFY\\x97\\x94\\xddg_\\xa6V.f\\xf4\\xf0p\\xce\\x8dt\\x92!\\xb159D\\x16\\xfb\\xac\\xa8\\xc9\\x05\\xf8\\xb3\\xf3~\\x11!D-\\xad\\x04\\xf1rY\\xa4H\\xe2\\x804\\x1e\\x01\\xba:RU\\xe4\\xe7\\x9d\\xc7G\\xd8-(\\x0e`\\xb6\\x9fM\\x18\\xbd\\x86\\xe1py.-\\xe5\\xbc\\x8a\\xcd;\\xdc7\\xeb^\\r\\xbf\\x99\\x05K\\x8b]\\x8b\\xbb4\\xe9\\xa8\\x98<*8l\\x9e\\xbfd{\\xead\\x14\\x08\\xd3\\xa3\\x1b\\x83v\\x92H\\xaa\\xc9Ox\\x1a\\xb2_\\xd6.\\x03\\x89\\xd0>\\t\\x08E\\xd2C\\xa4\\xaa\\xfeQ\\x97\\xff\\x1e\\x8c\\x9a\\xf5\\x97p\\xfa,L3'\n",
      "\n",
      "Model requested function: get_restaurant_rating\n",
      "Arguments: {'restaurant_name': 'Farmlore'}\n",
      "Function execution result: {'restaurant': 'Farmlore', 'rating': 4.8}\n",
      "\n",
      "Final model reply:\n",
      "The average rating for Farmlore in Bengaluru is 4.8.\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "# -------------------------------\n",
    "# Step 1: Define the function\n",
    "# -------------------------------\n",
    "restaurant_rating_function = {\n",
    "    \"name\": \"get_restaurant_rating\",\n",
    "    \"description\": \"Fetches the average rating for a given restaurant in Bengaluru.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"restaurant_name\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The name of the restaurant.\",\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"restaurant_name\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "def get_restaurant_rating(restaurant_name: str):\n",
    "    # Mock data – in real case, this could query Zomato/Google Maps API\n",
    "    mock_ratings = {\n",
    "        \"Farmlore\": 4.8,\n",
    "        \"Jamavar\": 4.6,\n",
    "        \"Karavalli\": 4.7,\n",
    "        \"ZLB23\": 4.5,\n",
    "        \"Copitas\": 4.6,\n",
    "        \"The Fatty Bao\": 4.4,\n",
    "        \"Rim Naam\": 4.7,\n",
    "        \"Mavalli Tiffin Room\": 4.3,\n",
    "        \"Yuki Cocktail Bar & Kitchen\": 4.5,\n",
    "        \"Lotus Pavilion\": 4.4,\n",
    "    }\n",
    "    return {\"restaurant\": restaurant_name, \"rating\": mock_ratings.get(restaurant_name, \"N/A\")}\n",
    "\n",
    "# -------------------------------\n",
    "# Step 2: Configure with thinking\n",
    "# -------------------------------\n",
    "tools = types.Tool(function_declarations=[restaurant_rating_function])\n",
    "\n",
    "config = types.GenerateContentConfig(\n",
    "    tools=[tools],\n",
    "    thinking_config=types.ThinkingConfig(\n",
    "        include_thoughts=True  # Ask the model to return its reasoning\n",
    "    ),\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Step 3: Ask the model\n",
    "# -------------------------------\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"What is the rating of Farmlore restaurant in Bengaluru?\",\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Step 4: Inspect response\n",
    "# -------------------------------\n",
    "for idx, part in enumerate(response.candidates[0].content.parts):\n",
    "    if part.thought_signature:\n",
    "        print(f\"Found thought signature in part {idx}: {part.thought_signature}\")\n",
    "\n",
    "    if part.function_call:\n",
    "        print(f\"\\nModel requested function: {part.function_call.name}\")\n",
    "        print(f\"Arguments: {part.function_call.args}\")\n",
    "\n",
    "        # Execute the function\n",
    "        result = get_restaurant_rating(**part.function_call.args)\n",
    "        print(f\"Function execution result: {result}\")\n",
    "\n",
    "        # -------------------------------\n",
    "        # Step 5: Send function result back\n",
    "        # -------------------------------\n",
    "        followup_response = client.models.generate_content(\n",
    "            model=\"gemini-2.5-flash\",\n",
    "            contents=[\n",
    "                response.candidates[0].content,  # include model’s function call turn\n",
    "                types.Content(\n",
    "                    role=\"function\",\n",
    "                    parts=[types.Part(\n",
    "                        function_response=types.FunctionResponse(\n",
    "                            name=part.function_call.name,\n",
    "                            response=result\n",
    "                        )\n",
    "                    )]\n",
    "                )\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        print(\"\\nFinal model reply:\")\n",
    "        print(followup_response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "880544e1-717f-4d6f-a764-71c015877c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "behavior=None description='Returns a * b.' name='multiply' parameters=Schema(\n",
      "  properties={\n",
      "    'a': Schema(\n",
      "      type=<Type.NUMBER: 'NUMBER'>\n",
      "    ),\n",
      "    'b': Schema(\n",
      "      type=<Type.NUMBER: 'NUMBER'>\n",
      "    )\n",
      "  },\n",
      "  required=[\n",
      "    'a',\n",
      "    'b',\n",
      "  ],\n",
      "  type=<Type.OBJECT: 'OBJECT'>\n",
      ") parameters_json_schema=None response=None response_json_schema=None\n",
      "{'description': 'Returns a * b.', 'name': 'multiply', 'parameters': {'properties': {'a': {'type': 'NUMBER'}, 'b': {'type': 'NUMBER'}}, 'required': ['a', 'b'], 'type': 'OBJECT'}}\n"
     ]
    }
   ],
   "source": [
    "# Automatic function schema declaration\n",
    "\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "def multiply(a: float, b: float):\n",
    "    \"\"\"Returns a * b.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "#  FunctionDeclarations from Python functions directly using types.FunctionDeclaration.from_callable(client=client, callable=your_function).\n",
    "fn_decl = types.FunctionDeclaration.from_callable(callable=multiply, client=client)\n",
    "\n",
    "print(fn_decl)\n",
    "# to_json_dict() provides a clean JSON representation.\n",
    "print(fn_decl.to_json_dict())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

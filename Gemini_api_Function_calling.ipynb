{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97ad0c53-ae4f-47cb-988f-b016d4955f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # take environment variables from .env.\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef54b7a2-359d-4991-9d2c-2d0a4e206f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function execution result: {'location': 'Bengaluru', 'temperature': '21°C'}\n",
      "Model’s natural language reply:\n",
      "The current temperature in Bengaluru is 21°C.\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "# Define the function declaration\n",
    "weather_function = {\n",
    "    \"name\": \"get_current_temperature\",\n",
    "    \"description\": \"Gets the current temperature for a given location.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"location\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The city name, e.g. San Francisco\",\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"location\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "def get_current_temperature(location):\n",
    "    return {\"location\": location, \"temperature\": \"21°C\"}\n",
    "\n",
    "# Client setup\n",
    "client = genai.Client()\n",
    "\n",
    "# tools object, which contains one or more function declarations\n",
    "tools = types.Tool(function_declarations=[weather_function])\n",
    "config = types.GenerateContentConfig(tools=[tools])\n",
    "\n",
    "# Step 1: User asks a question\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=[types.Content(role=\"user\", parts=[types.Part(text=\"What's the temperature in Bengaluru?\")])],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "tool_call = response.candidates[0].content.parts[0].function_call\n",
    "\n",
    "# Manual Pattern (When we reconstruct history)\n",
    "\n",
    "if tool_call and tool_call.name == \"get_current_temperature\":\n",
    "    result = get_current_temperature(**tool_call.args)\n",
    "    print(f\"Function execution result: {result}\")\n",
    "\n",
    "    # FIX: send function result back in same conversation\n",
    "    followup_response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=[\n",
    "        types.Content(\n",
    "            role=\"user\",\n",
    "            parts=[types.Part(text=\"What's the temperature in Bengaluru?\")]\n",
    "        ),\n",
    "        types.Content(\n",
    "            role=\"model\",\n",
    "            parts=[types.Part(function_call=tool_call)]\n",
    "        ),\n",
    "        types.Content(\n",
    "            role=\"function\",\n",
    "            parts=[types.Part(\n",
    "                function_response=types.FunctionResponse(\n",
    "                    name=tool_call.name,\n",
    "                    response=result\n",
    "                )\n",
    "            )]\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "    print(\"Model’s natural language reply:\")\n",
    "    print(followup_response.candidates[0].content.parts[0].text)\n",
    "\n",
    "else:\n",
    "    print(\"No function call found in the response.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3988047e-c7ae-4cba-873a-4ab2a6dfce7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To provide the most accurate list of top 10 restaurants in Bengaluru open today, October 5, 2025, I need to check the current operating status of highly-rated establishments. Based on recent culinary reviews and awards for 2025, several restaurants are consistently mentioned for their exceptional dining experiences.\n",
      "\n",
      "Here's a preliminary list of highly-regarded restaurants in Bengaluru from recent 2025 lists that I will now check for today's availability:\n",
      "\n",
      "*   **NAVU**\n",
      "*   **Farmlore**\n",
      "*   **Copitas**\n",
      "*   **Jamavar at The Leela Palace**\n",
      "*   **Karavalli**\n",
      "*   **ZLB23**\n",
      "*   **Naru Noodle Bar**\n",
      "*   **Bengaluru Oota Company**\n",
      "*   **Comal**\n",
      "*   **Marseli**\n",
      "*   **Mirth**\n",
      "*   **By The Blue**\n",
      "*   **Vesparo X Pulse**\n",
      "*   **Zarqash**\n",
      "*   **The Fatty Bao**\n",
      "*   **Black Pearl**\n",
      "*   **Hi Seoul**\n",
      "*   **Lupa**\n",
      "*   **Vidyarthi Bhavan**\n",
      "*   **Shiro**\n",
      "*   **JW Kitchen**\n",
      "*   **Kebabs and Kurries**\n",
      "*   **Rim Naam**\n",
      "*   **Kokoro**\n",
      "*   **Hunaaan**\n",
      "*   **Dali & Gala**\n",
      "\n",
      "I will now proceed to verify the operational status of these restaurants for October 5, 2025.Here are 10 highly-rated restaurants in Bengaluru that are confirmed to be open today, Saturday, October 5, 2025, along with their operating hours:\n",
      "\n",
      "1.  **JW Kitchen (JW Marriott Hotel Bengaluru)**: This all-day dining restaurant offers international cuisine. It is open today from 6:30 AM to 11:00 PM.\n",
      "2.  **The Fatty Bao (Indiranagar)**: Known for its Asian and Japanese cuisine, The Fatty Bao is open for lunch from 12:00 PM to 3:00 PM and for dinner from 7:00 PM to 10:30 PM.\n",
      "3.  **Karavalli (Vivanta Bengaluru, Residency Road)**: This award-winning restaurant specializes in South Western Coastal Indian cuisine. It operates from 12:30 PM to 3:00 PM and then from 7:00 PM to 11:30 PM.\n",
      "4.  **Rim Naam (The Oberoi, MG Road)**: Offering authentic Thai cuisine, Rim Naam is open for lunch from 12:30 PM to 3:00 PM and for dinner from 7:00 PM to 11:30 PM.\n",
      "5.  **Vidyarthi Bhavan (Basavanagudi)**: A legendary spot for South Indian vegetarian tiffin, particularly famous for its masala dosa. It is open today from 6:30 AM to 12:00 PM and then from 2:30 PM to 8:00 PM.\n",
      "6.  **The Black Pearl (Koramangala 5th Block)**: This pirate-themed restaurant serves a blend of Indian, Chinese, and Continental cuisines. It is open from 12:30 PM to 3:30 PM and from 6:30 PM to 11:45 PM. There is also a Marathahalli location with similar hours and a Rajajinagar location open from 12:00 PM to 3:30 PM and 6:30 PM to 11:00 PM.\n",
      "7.  **Bengaluru Oota Company**: This restaurant focuses on hyper-local Karnataka cuisine, including Gowda and Mangalorean dishes. It operates from 12:30 PM to 3:00 PM for lunch and 7:00 PM to 10:30 PM for dinner.\n",
      "8.  **Naru Noodle Bar**: A popular spot for Japanese ramen and other dishes, it operates with specific slots for lunch (12:30 PM, 2:30 PM, 4:30 PM) and dinner (6:30 PM, 8:30 PM), and reservations are highly recommended and typically open on Mondays at 8 PM, booking out quickly.\n",
      "9.  **Copitas (Four Seasons Hotel Bengaluru)**: A glamorous cocktail bar on the 21st floor, Copitas is open daily from 5:00 PM to 1:00 AM.\n",
      "10. **Dali & Gala Cocktail Bar**: This bar, known for its Salvador Dali-inspired art and craft cocktails, is open from 12:00 PM to 1:00 AM.\n"
     ]
    }
   ],
   "source": [
    "# Function calling with thinking\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "grounding_tool = types.Tool(\n",
    "    google_search=types.GoogleSearch()\n",
    ")\n",
    "\n",
    "config = types.GenerateContentConfig(\n",
    "    tools=[grounding_tool],\n",
    "    thinking_config=types.ThinkingConfig(\n",
    "        include_thoughts=True  # ask model to return reasoning so we know how the model thinks to pick the function/tools\n",
    "    ),\n",
    ")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"List top 10 restaurants in bengaluru which is available in  today(10/05/2025) \",\n",
    "    config=config,\n",
    "    \n",
    ")\n",
    "\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1c80ff6-050c-43f3-a31f-ff0677b75d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspecting Thought Signatures- mostly it will be displayed when we make function calling, to tell the model what operation performed in before request\n",
    "\n",
    "\n",
    "\n",
    "import base64\n",
    "# After receiving a response from a model with thinking enabled\n",
    "# response = client.models.generate_content(...)\n",
    "\n",
    "# The signature is attached to the response part containing the function call\n",
    "\n",
    "\n",
    "for idx, part in enumerate(response.candidates[0].content.parts):\n",
    "    if part.thought_signature:\n",
    "        print(f\"Found thought signature in part {idx}: {part.thought_signature}\")\n",
    "\n",
    "        print(base64.b64encode(part.thought_signature).decode(\"utf-8\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "64bc7b03-e456-490b-b1a1-8b54c6410d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n"
     ]
    }
   ],
   "source": [
    "# Function calling with thinking\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class cost(BaseModel):\n",
    "    dish_cost: int\n",
    "client = genai.Client()\n",
    "\n",
    "\n",
    "def bhai_biryani(dish:str):\n",
    "    \"\"\" return the cost of the dish, where the dish and it's cost is stored as key value pair in menu dictionary\"\"\"\n",
    "    menu = {'chicken fried rice':100, 'biryani':90, 'half kabab':50}\n",
    "    return menu.get(dish,0)\n",
    "def KFC(dish:str):\n",
    "    \"\"\" return the dish cost(key value)\"\"\"\n",
    "    menu={'biryani':200,'fried chicken':600}\n",
    "    return menu.get(dish)\n",
    "\n",
    "bhai_biryani_decl = types.FunctionDeclaration.from_callable(callable=bhai_biryani, client=client)\n",
    "KFC_decl = types.FunctionDeclaration.from_callable(callable=KFC, client=client)\n",
    "\n",
    "func_tools = types.Tool(function_declarations=[bhai_biryani_decl, KFC_decl])\n",
    "\n",
    "\n",
    "config = types.GenerateContentConfig(\n",
    "    tools=[func_tools],\n",
    "    thinking_config=types.ThinkingConfig(\n",
    "        include_thoughts=True  \n",
    "    ),\n",
    "    \n",
    ")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    #contents=\"what is the total cost if i order two biryani from bhai_biryani  \",\n",
    "    contents=\"what is the cost of  biryani in bhai biryani  \",\n",
    "    config=config,\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "for part in response.candidates[0].content.parts:\n",
    "    if part.function_call:\n",
    "        func_name = part.function_call.name\n",
    "        func_args = part.function_call.args\n",
    "        # Call the corresponding Python function manually\n",
    "        if func_name == \"bhai_biryani\":\n",
    "            result_value = bhai_biryani(**func_args)\n",
    "        elif func_name == \"KFC\":\n",
    "            result_value = KFC(**func_args)\n",
    "        \n",
    "        # Wrap in Pydantic model for structured access\n",
    "        result = cost(dish_cost=result_value)\n",
    "        print(result.dish_cost)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6092427f-591c-4050-a43b-54601aed7942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rate of beef rice is 40 and curd rice is 50.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# parallel function calling\n",
    "\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "def vegeterian(dish:str)->int:\n",
    "    \"\"\" Get the rate of the vegeterian dish\n",
    "\n",
    "    Args:\n",
    "        dish : name of the vegeterian dish, e.g: sambar rice\n",
    "\n",
    "    Returns:\n",
    "        dictionary where the value is rate\n",
    "    \"\"\"\n",
    "    menu ={'curd rice':50,'sambar rice':50, 'parotta':40}\n",
    "    return {\"rate \": menu.get(dish,0)}\n",
    "\n",
    "\n",
    "def non_vegeterian(dish:str)->int:\n",
    "    \"\"\" Get the rate of the non vegeterian dish\n",
    "\n",
    "    Args:\n",
    "        dish : name of the non vegeterian dish, e.g: chicken rice\n",
    "\n",
    "    Returns:\n",
    "        dictionary where the value is rate\n",
    "    \"\"\"\n",
    "    menu ={'chicken rice':50,'mutton rice':50, 'beef rice':40}\n",
    "    return {\"rate \": menu.get(dish,0)}\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "\"\"\"\n",
    "# configure function calling mode, by default AUTO, ANY - prefer functions first\n",
    "tool_config = types.ToolConfig(\n",
    "    function_calling_config = types.FunctionCallingConfig(\n",
    "        mode = 'ANY', allowed_function_names = [\"vegeterian\",\"non_vegeterian\"]\n",
    "    )\n",
    ")\n",
    "\"\"\"\n",
    "config = types.GenerateContentConfig(\n",
    "\n",
    "    #passing directly the function itself\n",
    "    tools = [vegeterian,non_vegeterian],\n",
    "    #tool_config = tool_config,\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model = \"gemini-2.5-flash\",\n",
    "    contents = \"what is the rate of beef rice and curd rice\",\n",
    "    config = config,\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b461231b-bc64-43ec-a5e8-4af5e4ca9846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponse(\n",
       "  automatic_function_calling_history=[\n",
       "    UserContent(\n",
       "      parts=[\n",
       "        Part(\n",
       "          text='what is the rate of beef rice and curd rice'\n",
       "        ),\n",
       "      ],\n",
       "      role='user'\n",
       "    ),\n",
       "    Content(\n",
       "      parts=[\n",
       "        Part(\n",
       "          function_call=FunctionCall(\n",
       "            args={\n",
       "              <... Max depth ...>: <... Max depth ...>\n",
       "            },\n",
       "            name='non_vegeterian'\n",
       "          ),\n",
       "          thought_signature=b'\\n\\xf6\\x03\\x01\\xd1\\xed\\x8ao\\x10\\xbb\\x10W\\xcf\\xab\\x18\\t\\xf3d\\x93\\x1b\"%\\x1d\\xf0\\x10[i\\x0bz\\x9c\\x95\\x1cJ\\xa3\\x81\\x9a#\\xc5\\xc5\\xe0\\x1c\\x0c+p\\x06\\xa8\\x18\\xe2\\xa7Cw lh\\t\\xcc8\\xf0\\xbe\\xd3\\xf5/\\xc4\\x81\\xf96\\xe5\\xa0qy\\xfa\\x80t@@\\xc2\\x9d\\xcf\\x0b_F\\x94\\xd0n\\\\Nw\\xd8\\xfd\\xbeW\\xb530\\xf4\\xd5\\x04...'\n",
       "        ),\n",
       "        Part(\n",
       "          function_call=FunctionCall(\n",
       "            args={\n",
       "              <... Max depth ...>: <... Max depth ...>\n",
       "            },\n",
       "            name='vegeterian'\n",
       "          )\n",
       "        ),\n",
       "      ],\n",
       "      role='model'\n",
       "    ),\n",
       "    Content(\n",
       "      parts=[\n",
       "        Part(\n",
       "          function_response=FunctionResponse(\n",
       "            name='non_vegeterian',\n",
       "            response={\n",
       "              <... Max depth ...>: <... Max depth ...>\n",
       "            }\n",
       "          )\n",
       "        ),\n",
       "        Part(\n",
       "          function_response=FunctionResponse(\n",
       "            name='vegeterian',\n",
       "            response={\n",
       "              <... Max depth ...>: <... Max depth ...>\n",
       "            }\n",
       "          )\n",
       "        ),\n",
       "      ],\n",
       "      role='user'\n",
       "    ),\n",
       "  ],\n",
       "  candidates=[\n",
       "    Candidate(\n",
       "      content=Content(\n",
       "        parts=[\n",
       "          Part(\n",
       "            text='The rate of beef rice is 40 and curd rice is 50.'\n",
       "          ),\n",
       "        ],\n",
       "        role='model'\n",
       "      ),\n",
       "      finish_reason=<FinishReason.STOP: 'STOP'>,\n",
       "      index=0\n",
       "    ),\n",
       "  ],\n",
       "  model_version='gemini-2.5-flash',\n",
       "  response_id='dv7jaMenGeuAvr0PtLep0Ag',\n",
       "  sdk_http_response=HttpResponse(\n",
       "    headers=<dict len=11>\n",
       "  ),\n",
       "  usage_metadata=GenerateContentResponseUsageMetadata(\n",
       "    candidates_token_count=17,\n",
       "    prompt_token_count=229,\n",
       "    prompt_tokens_details=[\n",
       "      ModalityTokenCount(\n",
       "        modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "        token_count=229\n",
       "      ),\n",
       "    ],\n",
       "    total_token_count=246\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fe6af61a-db5c-4f65-b84b-8949dddafe9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['thought_signature', 'function_call', 'function_call'], returning concatenated parsed result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rate of beef rice is 40 and curd rice is 50.\n",
      "beef rice: 40\n",
      "curd rice: 50\n"
     ]
    }
   ],
   "source": [
    "# parallel function calling - how can i add structured output(pydantic) in this code:\n",
    "\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class dishRate(BaseModel):\n",
    "    dish:str\n",
    "    rate:int\n",
    "\n",
    "def vegeterian(dish:str)->int:\n",
    "    \"\"\" Get the rate of the vegeterian dish\n",
    "\n",
    "    Args:\n",
    "        dish : name of the vegeterian dish, e.g: sambar rice\n",
    "\n",
    "    Returns:\n",
    "        dishRate object which contains dish name and dish rate\n",
    "    \"\"\"\n",
    "    menu ={'curd rice':50,'sambar rice':50, 'parotta':40}\n",
    "    return dishRate(dish=dish, rate = menu.get(dish,0))\n",
    "\n",
    "\n",
    "def non_vegeterian(dish:str)->int:\n",
    "    \"\"\" Get the rate of the non vegeterian dish\n",
    "\n",
    "    Args:\n",
    "        dish : name of the non vegeterian dish, e.g: chicken rice\n",
    "\n",
    "    Returns:\n",
    "        dishRate object which contains dish name and dish rate\n",
    "    \"\"\"\n",
    "    menu ={'chicken rice':50,'mutton rice':50, 'beef rice':40}\n",
    "    return dishRate(dish=dish, rate = menu.get(dish,0))\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "\"\"\"\n",
    "# configure function calling mode, by default AUTO, ANY - prefer functions first\n",
    "tool_config = types.ToolConfig(\n",
    "    function_calling_config = types.FunctionCallingConfig(\n",
    "        mode = 'ANY', allowed_function_names = [\"vegeterian\",\"non_vegeterian\"]\n",
    "    )\n",
    ")\n",
    "\"\"\"\n",
    "# for strutured output the function declaration is optimal(if i provide here, automatic function calling is not take place)\n",
    "veg_decl = types.FunctionDeclaration.from_callable(callable = vegeterian, client=client)\n",
    "nonveg_decl = types.FunctionDeclaration.from_callable(callable = non_vegeterian,client=client)\n",
    "tools = types.Tool(function_declarations=[veg_decl,nonveg_decl])\n",
    "\n",
    "config = types.GenerateContentConfig(\n",
    "    \n",
    "    #tools = [tools],\n",
    "    tools = [vegeterian,non_vegeterian],\n",
    "    automatic_function_calling=types.AutomaticFunctionCallingConfig(),\n",
    "    thinking_config=types.ThinkingConfig(include_thoughts=True),\n",
    "    response_schema = list[dishRate],\n",
    "    #response_mime_type = \"application/json\",\n",
    "    \n",
    "    #tool_config = tool_config,\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model = \"gemini-2.5-flash\",\n",
    "    contents = \"what is the rate of beef rice and curd rice\",\n",
    "    config = config,\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "print(response)\n",
    "print(response.text)\n",
    "structured_result: list[dishRate] = response.parsed\n",
    "for dish in structured_result:\n",
    "    print(f\"{dish.dish}: {dish.rate}\")\n",
    "\"\"\"\n",
    "print(response.text)\n",
    "results = []\n",
    "\n",
    "# iterate over all automatic_function_calling_history items\n",
    "for history_item in response.automatic_function_calling_history:\n",
    "    if hasattr(history_item, \"parts\"):\n",
    "        for part in history_item.parts:\n",
    "            if part.function_response:\n",
    "                func_result = part.function_response.response.get(\"result\")\n",
    "                if func_result:\n",
    "                    results.append(func_result)\n",
    "\n",
    "# results now contains DishRate objects\n",
    "for dish in results:\n",
    "    print(f\"{dish.dish}: {dish.rate}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eab107b-532d-4a95-bd88-497ca718ea57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f444735a-8eed-45c1-b6e5-d0bdd39c9c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['thought_signature', 'function_call', 'function_call'], returning concatenated parsed result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
      "Warning: there are non-text parts in the response: ['thought_signature'], returning concatenated parsed result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
      "Warning: there are non-text parts in the response: ['thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model did not return a structured response.\n",
      "The rate of beef rice is 40 and curd rice is 50.\n"
     ]
    }
   ],
   "source": [
    "# parallel function calling \n",
    "\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from pydantic import BaseModel\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "class dishRate(BaseModel):\n",
    "    dish:str\n",
    "    rate:int\n",
    "class allRates(BaseModel):\n",
    "    dishes:list[dishRate]\n",
    "\n",
    "def vegeterian(dish:str)->dishRate:\n",
    "    \"\"\" Get the rate of the vegeterian dish\n",
    "    \"\"\"\n",
    "    menu ={'curd rice':50,'sambar rice':50, 'parotta':40}\n",
    "    return dishRate(dish=dish, rate = menu.get(dish,0))\n",
    "\n",
    "\n",
    "def non_vegeterian(dish:str)->dishRate:\n",
    "    \"\"\" Get the rate of the non vegeterian dish\n",
    "    \"\"\"\n",
    "    menu ={'chicken rice':50,'mutton rice':50, 'beef rice':40}\n",
    "    return dishRate(dish=dish, rate = menu.get(dish,0))\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# configure function calling mode, by default AUTO, ANY - prefer functions first\n",
    "tool_config = types.ToolConfig(\n",
    "    function_calling_config = types.FunctionCallingConfig(mode = 'ANY')\n",
    ")\n",
    "\n",
    "# for strutured output the function declaration is optimal(if i provide here, automatic function calling is not take place)\n",
    "#veg_decl = types.FunctionDeclaration.from_callable(callable = vegeterian, client=client)\n",
    "#nonveg_decl = types.FunctionDeclaration.from_callable(callable = non_vegeterian,client=client)\n",
    "#tool = types.Tool(function_declarations=[veg_decl,nonveg_decl])\n",
    "async def main():\n",
    "    config = types.GenerateContentConfig(\n",
    "        \n",
    "        #tools = [tool],\n",
    "        tools = [vegeterian,non_vegeterian],\n",
    "        automatic_function_calling=types.AutomaticFunctionCallingConfig( ),\n",
    "        thinking_config=types.ThinkingConfig(include_thoughts=True),\n",
    "        #response_schema = allRates,\n",
    "        #response_mime_type = \"application/json\",\n",
    "        response_schema=allRates,\n",
    "        tool_config = types.ToolConfig(\n",
    "            function_calling_config=types.FunctionCallingConfig(mode=\"AUTO\")\n",
    "        ),\n",
    "        \n",
    "    )\n",
    "\n",
    "    response = await client.aio.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        #contents=\"what is the rate of beef rice(non veg) and curd rice(veg)\",\n",
    "        contents =\"what is the rate of beef rice(non veg) and curd rice(veg)\" ,\n",
    "\n",
    "        config=config,\n",
    "    )\n",
    "    structured_result: allRates = response.parsed\n",
    "    if response.parsed:\n",
    "        for dish in response.parsed.dishes:\n",
    "            print(f\"{dish.dish}: {dish.rate}\")\n",
    "    else:\n",
    "        print(\"Model did not return a structured response.\")\n",
    "        print(response.text)\n",
    "    \n",
    "\n",
    "\n",
    "await main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dc6ce23b-ea7d-45a3-89ed-6117f14b6577",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The price for chicken pizza is 100 and the GST for it is 10.\n"
     ]
    }
   ],
   "source": [
    "# compositional function calling\n",
    "\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import json\n",
    "def pizza_bakery_mani(flavour:str)->dict:\n",
    "    \"\"\" Get the price for a given given flavour\"\"\"\n",
    "    menu = {'chicken':100,'tomato':70,'corn':50,'beef':80}\n",
    "    return {'flavour':flavour,'price':menu.get(flavour)}\n",
    "\n",
    "def pizza_bakery_mani_gst(price:int)->dict:\n",
    "    \"\"\" return the gst for this price\"\"\"\n",
    "    return {\"gst_price\":price*0.10}\n",
    "\n",
    "client = genai.Client()\n",
    "config = types.GenerateContentConfig(\n",
    "    tools=[pizza_bakery_mani, pizza_bakery_mani_gst ]\n",
    ")\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"Get me the price and gst for chicken(flavour) pizza from pizza bakery mani\",\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e12bbf2-bcde-47b8-8062-a42a2465b4af",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 82\u001b[39m\n\u001b[32m     77\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m dish \u001b[38;5;129;01min\u001b[39;00m structured_result.dishes:\n\u001b[32m     78\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdish.dish\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdish.rate\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     84\u001b[39m \u001b[33;03mprint(response.text)\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[33;03mresults = []\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     99\u001b[39m \n\u001b[32m    100\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\runners.py:186\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[32m    162\u001b[39m \n\u001b[32m    163\u001b[39m \u001b[33;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    182\u001b[39m \u001b[33;03m    asyncio.run(main())\u001b[39;00m\n\u001b[32m    183\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m events._get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    185\u001b[39m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    187\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug=debug) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[32m    190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m runner.run(main)\n",
      "\u001b[31mRuntimeError\u001b[39m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "# parallel function calling \n",
    "\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from pydantic import BaseModel\n",
    "import asyncio\n",
    "\n",
    "\n",
    "class dishRate(BaseModel):\n",
    "    dish:str\n",
    "    rate:int\n",
    "class allRates(BaseModel):\n",
    "    dishes:list[dishRate]\n",
    "\n",
    "def vegeterian(dish:str)->int:\n",
    "    \"\"\" Get the rate of the vegeterian dish\n",
    "    \"\"\"\n",
    "    menu ={'curd rice':50,'sambar rice':50, 'parotta':40}\n",
    "    return dishRate(dish=dish, rate = menu.get(dish,0))\n",
    "\n",
    "\n",
    "def non_vegeterian(dish:str)->int:\n",
    "    \"\"\" Get the rate of the non vegeterian dish\n",
    "    \"\"\"\n",
    "    menu ={'chicken rice':50,'mutton rice':50, 'beef rice':40}\n",
    "    return dishRate(dish=dish, rate = menu.get(dish,0))\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# configure function calling mode, by default AUTO, ANY - prefer functions first\n",
    "tool_config = types.ToolConfig(\n",
    "    function_calling_config = types.FunctionCallingConfig(mode = 'ANY')\n",
    ")\n",
    "\n",
    "# for strutured output the function declaration is optimal(if i provide here, automatic function calling is not take place)\n",
    "#veg_decl = types.FunctionDeclaration.from_callable(callable = vegeterian, client=client)\n",
    "#nonveg_decl = types.FunctionDeclaration.from_callable(callable = non_vegeterian,client=client)\n",
    "#tool = types.Tool(function_declarations=[veg_decl,nonveg_decl])\n",
    "\n",
    "config = types.GenerateContentConfig(\n",
    "    \n",
    "    #tools = [tool],\n",
    "    tools = [vegeterian,non_vegeterian],\n",
    "    automatic_function_calling=types.AutomaticFunctionCallingConfig(),\n",
    "    thinking_config=types.ThinkingConfig(include_thoughts=True),\n",
    "    response_schema = allRates,\n",
    "    #response_mime_type = \"application/json\",\n",
    "    \n",
    "    tool_config = types.ToolConfig(\n",
    "        function_calling_config=types.FunctionCallingConfig(mode=\"ANY\")\n",
    "    ),\n",
    "    \n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "response = client.models.generate_content(\n",
    "    model = \"gemini-2.5-flash\",\n",
    "    contents = \"what is the rate of beef rice(non veg) and curd rice(veg)\",\n",
    "    config = config,\n",
    ")\n",
    "\"\"\"\n",
    "async def main():\n",
    "    response = await client.aio.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=\"what is the rate of beef rice(non veg) and curd rice(veg)\",\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "    print(\"Automatic Function Calls:\\n\", response.automatic_function_calling_history)\n",
    "    \n",
    "    print(response)\n",
    "    #print(response.text)\n",
    "    structured_result: allRates = response.parsed\n",
    "    for dish in structured_result.dishes:\n",
    "        print(f\"{dish.dish}: {dish.rate}\")\n",
    "\n",
    "\n",
    "\n",
    "asyncio.run(main())\n",
    "\"\"\"\n",
    "print(response.text)\n",
    "results = []\n",
    "\n",
    "# iterate over all automatic_function_calling_history items\n",
    "for history_item in response.automatic_function_calling_history:\n",
    "    if hasattr(history_item, \"parts\"):\n",
    "        for part in history_item.parts:\n",
    "            if part.function_response:\n",
    "                func_result = part.function_response.response.get(\"result\")\n",
    "                if func_result:\n",
    "                    results.append(func_result)\n",
    "\n",
    "# results now contains DishRate objects\n",
    "for dish in results:\n",
    "    print(f\"{dish.dish}: {dish.rate}\")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "33a7b68e-44b6-46cf-9fe9-eb2ad633d1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['thought_signature', 'function_call'], returning concatenated parsed result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool Call: get_weather_forecast(location=London)\n",
      "Tool Response: {'temperature': 25, 'unit': 'celsius'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated parsed result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool Call: set_thermostat_temperature(temperature=20)\n",
      "Tool Response: {'status': 'success'}\n",
      "Model did not return structured output.\n",
      "I have set the thermostat to 20°C.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# -----------------------------\n",
    "# 1️⃣ Define Pydantic schema\n",
    "# -----------------------------\n",
    "class ThermostatResult(BaseModel):\n",
    "    location: str\n",
    "    current_temperature: int\n",
    "    thermostat_setting: int\n",
    "    status: str\n",
    "\n",
    "# -----------------------------\n",
    "# 2️⃣ Define functions\n",
    "# -----------------------------\n",
    "def get_weather_forecast(location: str) -> dict:\n",
    "    \"\"\"Gets the current weather temperature for a given location.\"\"\"\n",
    "    print(f\"Tool Call: get_weather_forecast(location={location})\")\n",
    "    print(\"Tool Response: {'temperature': 25, 'unit': 'celsius'}\")\n",
    "    return {\"temperature\": 25, \"unit\": \"celsius\"}  # Dummy response\n",
    "\n",
    "def set_thermostat_temperature(temperature: int) -> dict:\n",
    "    \"\"\"Sets the thermostat to a desired temperature.\"\"\"\n",
    "    print(f\"Tool Call: set_thermostat_temperature(temperature={temperature})\")\n",
    "    print(\"Tool Response: {'status': 'success'}\")\n",
    "    return {\"status\": \"success\"}\n",
    "\n",
    "# -----------------------------\n",
    "# 3️⃣ Configure client + schema\n",
    "# -----------------------------\n",
    "client = genai.Client()\n",
    "\n",
    "config = types.GenerateContentConfig(\n",
    "    tools=[get_weather_forecast, set_thermostat_temperature],\n",
    "    automatic_function_calling=types.AutomaticFunctionCallingConfig(),\n",
    "    response_schema=ThermostatResult,   # 👈 NEW: structured output type\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 4️⃣ Run model\n",
    "# -----------------------------\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"If it's warmer than 20°C in London, set the thermostat to 20°C, otherwise set it to 18°C.\",\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 5️⃣ Access structured output\n",
    "# -----------------------------\n",
    "if response.parsed:\n",
    "    result: ThermostatResult = response.parsed\n",
    "    print(\"✅ Structured Result:\")\n",
    "    print(result.json(indent=2))\n",
    "\n",
    "    # You can reuse later like:\n",
    "    print(\"\\nLater use example:\")\n",
    "    print(f\"Thermostat was set to {result.thermostat_setting}°C in {result.location}\")\n",
    "else:\n",
    "    print(\"Model did not return structured output.\")\n",
    "    print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4bebaf3c-9385-4ef7-9999-45af87bc296d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found thought signature in part 1: b'\\n\\xa7\\x02\\x01\\xd1\\xed\\x8ao\\x83\\x12\\x97\\xea+\\xfc5,\\xe1:\\xbd\\xd2T-\\x04Ux\\'\\x8cLbs\\x96\\xff\\xcaT\\x03\\\\\\x9f\\xd7\\x93=\\nTM\\x93\\x01OgNS\\\\\\x0e\\xa7\\xd1\\x02c\\xea\\x1e\\xd9\\x06\\xac\\xb0~\\x93\\xae\\xe2^\\x85L=\\xb0\\xcd\\x9a\\x08x\\x91\\xd7\\xa4F\\xa6e\\xf2\\xe2\\xc4\\x02\\xa4\\xf4\\xf0\\x13P\\x8e\\xbd\\x88TL\\xdef\\xa9\\x8aW&\\xbb\\x15\\xad\\x87g\\xb1\\x10\\x87\\xacM\\xe1\\xaed2T\\x84\\x98.\\x16\\xa5\\xc8\\xd5\\x0bQ\\x91\\xa0;\\xb3\\xa0\\x06\\xdc\\x08\\xec\\xb7\\x98\\xde\\xf9\\xd2\\x00#\\xb7\\xe6\\x82\\xfb\"\\xbej\\x9dyFY\\x97\\x94\\xddg_\\xa6V.f\\xf4\\xf0p\\xce\\x8dt\\x92!\\xb159D\\x16\\xfb\\xac\\xa8\\xc9\\x05\\xf8\\xb3\\xf3~\\x11!D-\\xad\\x04\\xf1rY\\xa4H\\xe2\\x804\\x1e\\x01\\xba:RU\\xe4\\xe7\\x9d\\xc7G\\xd8-(\\x0e`\\xb6\\x9fM\\x18\\xbd\\x86\\xe1py.-\\xe5\\xbc\\x8a\\xcd;\\xdc7\\xeb^\\r\\xbf\\x99\\x05K\\x8b]\\x8b\\xbb4\\xe9\\xa8\\x98<*8l\\x9e\\xbfd{\\xead\\x14\\x08\\xd3\\xa3\\x1b\\x83v\\x92H\\xaa\\xc9Ox\\x1a\\xb2_\\xd6.\\x03\\x89\\xd0>\\t\\x08E\\xd2C\\xa4\\xaa\\xfeQ\\x97\\xff\\x1e\\x8c\\x9a\\xf5\\x97p\\xfa,L3'\n",
      "\n",
      "Model requested function: get_restaurant_rating\n",
      "Arguments: {'restaurant_name': 'Farmlore'}\n",
      "Function execution result: {'restaurant': 'Farmlore', 'rating': 4.8}\n",
      "\n",
      "Final model reply:\n",
      "The average rating for Farmlore in Bengaluru is 4.8.\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "# -------------------------------\n",
    "# Step 1: Define the function\n",
    "# -------------------------------\n",
    "restaurant_rating_function = {\n",
    "    \"name\": \"get_restaurant_rating\",\n",
    "    \"description\": \"Fetches the average rating for a given restaurant in Bengaluru.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"restaurant_name\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The name of the restaurant.\",\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"restaurant_name\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "def get_restaurant_rating(restaurant_name: str):\n",
    "    # Mock data – in real case, this could query Zomato/Google Maps API\n",
    "    mock_ratings = {\n",
    "        \"Farmlore\": 4.8,\n",
    "        \"Jamavar\": 4.6,\n",
    "        \"Karavalli\": 4.7,\n",
    "        \"ZLB23\": 4.5,\n",
    "        \"Copitas\": 4.6,\n",
    "        \"The Fatty Bao\": 4.4,\n",
    "        \"Rim Naam\": 4.7,\n",
    "        \"Mavalli Tiffin Room\": 4.3,\n",
    "        \"Yuki Cocktail Bar & Kitchen\": 4.5,\n",
    "        \"Lotus Pavilion\": 4.4,\n",
    "    }\n",
    "    return {\"restaurant\": restaurant_name, \"rating\": mock_ratings.get(restaurant_name, \"N/A\")}\n",
    "\n",
    "# -------------------------------\n",
    "# Step 2: Configure with thinking\n",
    "# -------------------------------\n",
    "tools = types.Tool(function_declarations=[restaurant_rating_function])\n",
    "\n",
    "config = types.GenerateContentConfig(\n",
    "    tools=[tools],\n",
    "    thinking_config=types.ThinkingConfig(\n",
    "        include_thoughts=True  # Ask the model to return its reasoning\n",
    "    ),\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Step 3: Ask the model\n",
    "# -------------------------------\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"What is the rating of Farmlore restaurant in Bengaluru?\",\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Step 4: Inspect response\n",
    "# -------------------------------\n",
    "for idx, part in enumerate(response.candidates[0].content.parts):\n",
    "    if part.thought_signature:\n",
    "        print(f\"Found thought signature in part {idx}: {part.thought_signature}\")\n",
    "\n",
    "    if part.function_call:\n",
    "        print(f\"\\nModel requested function: {part.function_call.name}\")\n",
    "        print(f\"Arguments: {part.function_call.args}\")\n",
    "\n",
    "        # Execute the function\n",
    "        result = get_restaurant_rating(**part.function_call.args)\n",
    "        print(f\"Function execution result: {result}\")\n",
    "\n",
    "        # -------------------------------\n",
    "        # Step 5: Send function result back\n",
    "        # -------------------------------\n",
    "        followup_response = client.models.generate_content(\n",
    "            model=\"gemini-2.5-flash\",\n",
    "            contents=[\n",
    "                response.candidates[0].content,  # include model’s function call turn\n",
    "                types.Content(\n",
    "                    role=\"function\",\n",
    "                    parts=[types.Part(\n",
    "                        function_response=types.FunctionResponse(\n",
    "                            name=part.function_call.name,\n",
    "                            response=result\n",
    "                        )\n",
    "                    )]\n",
    "                )\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        print(\"\\nFinal model reply:\")\n",
    "        print(followup_response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "880544e1-717f-4d6f-a764-71c015877c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "behavior=None description='Returns a * b.' name='multiply' parameters=Schema(\n",
      "  properties={\n",
      "    'a': Schema(\n",
      "      type=<Type.NUMBER: 'NUMBER'>\n",
      "    ),\n",
      "    'b': Schema(\n",
      "      type=<Type.NUMBER: 'NUMBER'>\n",
      "    )\n",
      "  },\n",
      "  required=[\n",
      "    'a',\n",
      "    'b',\n",
      "  ],\n",
      "  type=<Type.OBJECT: 'OBJECT'>\n",
      ") parameters_json_schema=None response=None response_json_schema=None\n",
      "{'description': 'Returns a * b.', 'name': 'multiply', 'parameters': {'properties': {'a': {'type': 'NUMBER'}, 'b': {'type': 'NUMBER'}}, 'required': ['a', 'b'], 'type': 'OBJECT'}}\n"
     ]
    }
   ],
   "source": [
    "# Automatic function schema declaration\n",
    "\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "def multiply(a: float, b: float):\n",
    "    \"\"\"Returns a * b.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "#  FunctionDeclarations from Python functions directly using types.FunctionDeclaration.from_callable(client=client, callable=your_function).\n",
    "fn_decl = types.FunctionDeclaration.from_callable(callable=multiply, client=client)\n",
    "\n",
    "print(fn_decl)\n",
    "# to_json_dict() provides a clean JSON representation.\n",
    "print(fn_decl.to_json_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9d81d2-2113-4700-ad3f-fb3f5eacdb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-tool use: Combine native tools with function calling\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

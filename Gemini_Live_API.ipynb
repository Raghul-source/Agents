{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74674b64-dd49-4bb8-b5d5-4194c37e347c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # take environment variables from .env.\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "439a16b7-9eb5-423f-aea0-23c290e3f0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['executable_code'], returning concatenated text result from text parts, check out the non text parts for full response from model.\n",
      "Warning: there are non-text parts in the response: ['code_execution_result'], returning concatenated text result from text parts, check out the non text parts for full response from model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shop is now open."
     ]
    }
   ],
   "source": [
    "# gemini live api with async and custom function call dynamically\n",
    "\n",
    "import asyncio \n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "client = genai.Client()\n",
    "model = \"gemini-live-2.5-flash-preview\"\n",
    "\n",
    "\n",
    "def open_shop():\n",
    "    return \"opened\"\n",
    "\n",
    "def close_shop():\n",
    "    return \"closed\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "shop_open = types.FunctionDeclaration(name=\"open_shop\",behavior=\"NON_BLOCKING\")\n",
    "shop_close = types.FunctionDeclaration(name=\"close_shop\")\n",
    "\n",
    "\n",
    "tools = [types.Tool(function_declarations=[shop_open, shop_close])]\n",
    "\n",
    "\n",
    "\n",
    "config = types.LiveConnectConfig(\n",
    "     response_modalities = [\"TEXT\"],\n",
    "     tools=tools\n",
    ")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    async with client.aio.live.connect(model=model, config = config) as session:\n",
    "        prompt = \"open the shop and tell me what happened\"\n",
    "        await session.send_client_content(turns ={\"parts\":[{\"text\":prompt}]})\n",
    "\n",
    "        async for chunk in session.receive():\n",
    "            #print(chunk)\n",
    "            if chunk.server_content:\n",
    "                if chunk.text is not None:\n",
    "                    print(chunk.text,end=\"\")\n",
    "            if chunk.tool_call:\n",
    "                function_responses = []\n",
    "                for fc in chunk.tool_call.function_calls:\n",
    "                    #print(fc)\n",
    "                    result = open_shop() if fc.name == \"open_shop\" else close_shop()\n",
    "                    function_response = types.FunctionResponse(\n",
    "                        id = fc.id,\n",
    "                        name = fc.name,\n",
    "                        response = {\"result\":result,\"scheduling\":\"WHEN_IDLE\"}\n",
    "                    )\n",
    "                    function_responses.append(function_response)\n",
    "                await session.send_tool_response(function_responses=function_responses)\n",
    "                #await session.send_tool_response(function_responses)\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6881a0f-3ce5-4bc0-ac76-129a352361ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A  To\n",
      "A   detect faces using OpenCV, you typically follow these steps:\n",
      "\n",
      "1.  **\n",
      "A  Load the pre-trained Haar Cascade classifier:** OpenCV provides pre-trained Haar Cascades for various object detections, including frontal faces.\n",
      "2.  **Load the image:** Read the image on which you want to perform face detection.\n",
      "3.  \n",
      "A  **Convert to grayscale:** Haar Cascades work best on grayscale images, as color information is not crucial for feature detection in this method.\n",
      "4.  **Perform face detection:** Use the loaded classifier to detect faces in the grayscale image. This function returns\n",
      "A   a list of rectangles, where each rectangle represents a detected face.\n",
      "5.  **Draw bounding boxes:** Iterate through the detected faces and draw a rectangle (bounding box) around each face on the original image.\n",
      "6.  **Display\n",
      "A   the image:** Show the image with the detected faces.\n",
      "\n",
      "Here's the Python code to achieve this:\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['executable_code'], returning concatenated text result from text parts, check out the non text parts for full response from model.\n",
      "Warning: there are non-text parts in the response: ['code_execution_result'], returning concatenated text result from text parts, check out the non text parts for full response from model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B  import cv2\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# 1. Load the pre-trained Haar Cascade classifier for face detection\n",
      "# You need to have the 'haarcascade_frontalface_default.xml' file\n",
      "# in the same directory as your script, or provide the full path to it.\n",
      "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
      "\n",
      "# Check if the cascade classifier loaded successfully\n",
      "if face_cascade.empty():\n",
      "    print(\"Error: Could not load face cascade classifier.\")\n",
      "else:\n",
      "    print(\"Face cascade classifier loaded successfully.\")\n",
      "\n",
      "    # Create a dummy image for demonstration\n",
      "    # In a real scenario, you would load an image from a file or camera.\n",
      "    # For now, let's create a black image and pretend it has a face.\n",
      "    # A better approach for demonstration is to use an image URL or a sample image.\n",
      "    # Let's use a sample image that we can download.\n",
      "    # For a real application, you would replace this with your image loading logic.\n",
      "\n",
      "    # If you have an image file, uncomment and modify the line below:\n",
      "    # image_path = 'your_image.jpg'\n",
      "    # img = cv2.imread(image_path)\n",
      "\n",
      "    # For demonstration, let's try to fetch an image (requires internet)\n",
      "    # If this fails, you might need to provide a local image.\n",
      "    try:\n",
      "        import urllib.request\n",
      "        url = \"https://upload.wikimedia.org/wikipedia/commons/8/8d/President_Barack_Obama.jpg\"\n",
      "        resp = urllib.request.urlopen(url)\n",
      "        image_array = bytearray(resp.read())\n",
      "        img = cv2.imdecode(np.array(image_array), cv2.IMREAD_COLOR)\n",
      "        print(\"Image downloaded successfully for demonstration.\")\n",
      "    except Exception as e:\n",
      "        print(f\"Could not download image for demonstration: {e}. Please provide a local image file.\")\n",
      "        # Create a blank image if download fails, so the script doesn't crash\n",
      "        import numpy as np\n",
      "        img = np.zeros((400, 600, 3), dtype=np.uint8)\n",
      "        cv2.putText(img, \"Please replace with your image.jpg\", (50, 200), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
      "\n",
      "\n",
      "    if img is None:\n",
      "        print(\"Error: Could not load image. Please check the image path or download method.\")\n",
      "    else:\n",
      "        # Resize image to fit screen if it's too large\n",
      "        scale_percent = 0.5 # percent of original size\n",
      "        width = int(img.shape[1] * scale_percent)\n",
      "        height = int(img.shape[0] * scale_percent)\n",
      "        dim = (width, height)\n",
      "        img_resized = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
      "\n",
      "        # 3. Convert to grayscale\n",
      "        gray = cv2.cvtColor(img_resized, cv2.COLOR_BGR2GRAY)\n",
      "\n",
      "        # 4. Perform face detection\n",
      "        # detectMultiScale(image, scaleFactor, minNeighbors)\n",
      "        # scaleFactor: Parameter specifying how much the image size is reduced at each image scale.\n",
      "        # minNeighbors: Parameter specifying how many neighbors each candidate rectangle should have to retain it.\n",
      "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
      "\n",
      "        print(f\"Found {len(faces)} faces.\")\n",
      "\n",
      "        # 5. Draw bounding boxes around the detected faces\n",
      "        for (x, y, w, h) in faces:\n",
      "            cv2.rectangle(img_resized, (x, y), (x+w, y+h), (255, 0, 0), 2) # Blue rectangle, thickness 2\n",
      "\n",
      "        # 6. Display the image with detected faces\n",
      "        # OpenCV uses BGR by default, Matplotlib expects RGB\n",
      "        img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)\n",
      "\n",
      "        plt.imshow(img_rgb)\n",
      "        plt.title('Face Detection')\n",
      "        plt.axis('off')\n",
      "        plt.show()\n",
      "\n",
      "        # Optionally, save the output image\n",
      "        # cv2.imwrite('face_detected_output.jpg', img_resized)\n",
      "        # print(\"Output image saved as 'face_detected_output.jpg'\")\n",
      "C  Traceback (most recent call last):\n",
      "  File \"<string>\", line 7, in <module>\n",
      "AttributeError: module 'cv2' has no attribute 'data'\n",
      "\n",
      "A  It\n",
      "A   seems like there was an issue with accessing `cv2.data.haarc\n",
      "A  ascades`. This attribute is used to get the path to the Haar cascade XML files that come with the OpenCV installation. If `cv2.data` is not available or `haarcascades` isn't accessible this way, it\n",
      "A   usually means there's a slight variation in the OpenCV installation or environment.\n",
      "\n",
      "Let's try to fix this by explicitly pointing to the XML file if `cv2.data.haarcascades` isn't working as expected. Sometimes\n",
      "A  , the `haarcascade_frontalface_default.xml` file might be located in a specific path within your OpenCV installation, or you might need to download it manually and place it in your project directory.\n",
      "\n",
      "A common approach\n",
      "A   is to search for the `haarcascade_frontalface_default.xml` file in your system or download it from the official OpenCV GitHub repository. For simplicity in this environment, I'll assume you have the file available, or\n",
      "A   provide a link to download it.\n",
      "\n",
      "If you are running this locally, you would typically find these files in a path similar to:\n",
      "`C:\\Users\\<YourUser>\\AppData\\Local\\Programs\\Python\\Python3x\\Lib\\site\n",
      "A  -packages\\cv2\\data\\` on Windows, or\n",
      "`/usr/local/lib/pythonX.X/site-packages/cv2/data/` on Linux/macOS.\n",
      "\n",
      "However, since I cannot directly access the file\n",
      "A   system in this environment to find the exact path, I will modify the code to try a common way to get the path, and if that fails, guide you on how to place the file manually.\n",
      "\n",
      "Let's try to make the `cv\n",
      "A  2.data.haarcascades` part more robust. If that still fails, the most reliable way is to download the `haarcascade_frontalface_default.xml` file from the official OpenCV GitHub repository (e.g.,\n",
      "A   [https://github.com/opencv/opencv/tree/master/data/haarcascades](https://github.com/opencv/opencv/tree/master/data/haarcascades)) and place it in the\n",
      "A   same directory as your Python script, then simply use its filename: `face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')`.\n",
      "\n",
      "Let's retry the code, making sure \n",
      "A  `numpy` is imported for the image download part and handling the `cv2.data` path more carefully. I'll also add a fallback for `cv2.data.haarcascades` by assuming the file is in the\n",
      "A   current directory as a common workaround for environments where the `data` attribute isn't directly exposed or the path isn't correctly configured.\n",
      "\n",
      "First, let's include `numpy` explicitly and try the `cv2.data.haarc\n",
      "A  ascades` again, and if that gives an `AttributeError`, we'll try to use a direct filename, implying the user has placed the file.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['executable_code'], returning concatenated text result from text parts, check out the non text parts for full response from model.\n",
      "Warning: there are non-text parts in the response: ['code_execution_result'], returning concatenated text result from text parts, check out the non text parts for full response from model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B  import cv2\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np # Import numpy\n",
      "\n",
      "# 1. Load the pre-trained Haar Cascade classifier for face detection\n",
      "# Try accessing the cascade from cv2.data, which is the standard way.\n",
      "# If it fails, we will assume the file is in the current directory.\n",
      "cascade_path = None\n",
      "try:\n",
      "    cascade_path = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
      "    face_cascade = cv2.CascadeClassifier(cascade_path)\n",
      "    if face_cascade.empty():\n",
      "        print(f\"Error: Could not load face cascade classifier from {cascade_path}.\")\n",
      "        # Fallback if loading from the standard path fails\n",
      "        print(\"Attempting to load from current directory: haarcascade_frontalface_default.xml\")\n",
      "        face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
      "except AttributeError:\n",
      "    print(\"cv2.data.haarcascades attribute not found. Attempting to load from current directory: haarcascade_frontalface_default.xml\")\n",
      "    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
      "except Exception as e:\n",
      "    print(f\"An unexpected error occurred while loading cascade: {e}\")\n",
      "    face_cascade = None # Ensure face_cascade is None if any error occurs\n",
      "\n",
      "# Check if the cascade classifier loaded successfully (after all attempts)\n",
      "if face_cascade is None or face_cascade.empty():\n",
      "    print(\"Final Error: Could not load face cascade classifier. Please ensure 'haarcascade_frontalface_default.xml' is in the current directory or accessible via cv2.data.haarcascades.\")\n",
      "else:\n",
      "    print(\"Face cascade classifier loaded successfully.\")\n",
      "\n",
      "    # Try to download a sample image for demonstration\n",
      "    img = None\n",
      "    try:\n",
      "        import urllib.request\n",
      "        url = \"https://upload.wikimedia.org/wikipedia/commons/8/8d/President_Barack_Obama.jpg\"\n",
      "        resp = urllib.request.urlopen(url)\n",
      "        image_array = bytearray(resp.read())\n",
      "        img = cv2.imdecode(np.array(image_array), cv2.IMREAD_COLOR)\n",
      "        print(\"Sample image downloaded successfully for demonstration.\")\n",
      "    except Exception as e:\n",
      "        print(f\"Could not download sample image for demonstration: {e}.\")\n",
      "        print(\"Please provide a local image file (e.g., 'your_image.jpg') in the same directory.\")\n",
      "        # Create a blank image if download fails\n",
      "        img = np.zeros((400, 600, 3), dtype=np.uint8)\n",
      "        cv2.putText(img, \"Error: No image loaded. Replace with your_image.jpg\", (50, 200), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
      "\n",
      "\n",
      "    if img is None:\n",
      "        print(\"Error: Image could not be loaded or created.\")\n",
      "    else:\n",
      "        # Resize image to fit screen if it's too large\n",
      "        scale_percent = 0.5 # percent of original size\n",
      "        width = int(img.shape[1] * scale_percent)\n",
      "        height = int(img.shape[0] * scale_percent)\n",
      "        dim = (width, height)\n",
      "        img_resized = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
      "\n",
      "        # 3. Convert to grayscale\n",
      "        gray = cv2.cvtColor(img_resized, cv2.COLOR_BGR2GRAY)\n",
      "\n",
      "        # 4. Perform face detection\n",
      "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
      "\n",
      "        print(f\"Found {len(faces)} faces.\")\n",
      "\n",
      "        # 5. Draw bounding boxes around the detected faces\n",
      "        for (x, y, w, h) in faces:\n",
      "            cv2.rectangle(img_resized, (x, y), (x+w, y+h), (255, 0, 0), 2) # Blue rectangle, thickness 2\n",
      "\n",
      "        # 6. Display the image with detected faces\n",
      "        img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)\n",
      "\n",
      "        plt.imshow(img_rgb)\n",
      "        plt.title('Face Detection')\n",
      "        plt.axis('off')\n",
      "        plt.show()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['inline_data'], returning concatenated text result from text parts, check out the non text parts for full response from model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C  cv2.data.haarcascades attribute not found. Attempting to load from current directory: haarcascade_frontalface_default.xml\n",
      "Final Error: Could not load face cascade classifier. Please ensure 'haarcascade_frontalface_default.xml' is in the current directory or accessible via cv2.data.haarcascades.\n",
      "\n",
      "A  You\n",
      "A  're absolutely right! The previous attempt failed because the `haarcascade_frontal\n",
      "A  face_default.xml` file was not found, which is a common issue when running in environments where you can't easily manage external files or the `cv2.data` path isn't correctly configured.\n",
      "\n",
      "Since I cannot directly download\n",
      "A   and save files into your environment for you, I'll provide you with instructions on how to obtain the `haarcascade_frontalface_default.xml` file and then demonstrate how you would use it assuming it's present\n",
      "A  .\n",
      "\n",
      "**To make this code work on your end, you need to perform the following steps:**\n",
      "\n",
      "1.  **Download `haarcascade_frontalface_default.xml`:**\n",
      "    *   Go to the official\n",
      "A   OpenCV GitHub repository for Haar Cascades: [https://github.com/opencv/opencv/tree/master/data/haarcascades](https://github.com/opencv/opencv/tree/master/data/haarcascades\n",
      "A  )\n",
      "    *   Click on the `haarcascade_frontalface_default.xml` file.\n",
      "    *   Click the \"Raw\" button. This will display the XML content in your browser.\n",
      "    *   Right\n",
      "A  -click on the page (or use your browser's \"Save Page As...\" option) and save the file as `haarcascade_frontalface_default.xml` in the **same directory** where your Python script is located\n",
      "A  .\n",
      "\n",
      "2.  **Ensure you have an image file:**\n",
      "    *   The current code attempts to download an image from Wikimedia. If this fails due to network restrictions or other issues, you will need to provide a local image file.\n",
      "A  \n",
      "    *   Place a sample image (e.g., `test_image.jpg`) in the **same directory** as your Python script and the Haar cascade XML file.\n",
      "    *   Then, you'll uncomment and modify the \n",
      "A  `img = cv2.imread(...)` line to point to your local image.\n",
      "\n",
      "Now, let's execute the code again, but this time, **I will simulate the presence of the `haarcascade_frontalface_default\n",
      "A  .xml` file by loading a local file named `haarcascade_frontalface_default.xml` in the tool code and loading a local image `test_image.jpg`**.\n",
      "\n",
      "For the demonstration, I will also create\n",
      "A   a dummy `test_image.jpg` to ensure the image loading part works, and it will contain a simple rectangle as a \"face\" so the face detection algorithm has something to detect, as the actual cascade won't be applied\n",
      "A   without the XML.\n",
      "\n",
      "**Important Note for the User:** When you run this code, replace the dummy image creation and the assumption of `haarcascade_frontalface_default.xml` being magically there, with your downloaded `.\n",
      "A  xml` file and your actual image.\n",
      "\n",
      "Here's the revised code. I will create a dummy `haarcascade_frontalface_default.xml` file (though it won't be a valid cascade for face detection, it\n",
      "A   will allow `cv2.CascadeClassifier` to open *a* file) and a dummy image for the purpose of demonstrating the flow.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['executable_code'], returning concatenated text result from text parts, check out the non text parts for full response from model.\n",
      "Warning: there are non-text parts in the response: ['code_execution_result'], returning concatenated text result from text parts, check out the non text parts for full response from model.\n",
      "Warning: there are non-text parts in the response: ['inline_data'], returning concatenated text result from text parts, check out the non text parts for full response from model.\n",
      "Warning: there are non-text parts in the response: ['inline_data'], returning concatenated text result from text parts, check out the non text parts for full response from model.\n",
      "Warning: there are non-text parts in the response: ['inline_data'], returning concatenated text result from text parts, check out the non text parts for full response from model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B  import cv2\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import os # To check for file existence and create dummy files\n",
      "\n",
      "# --- Dummy File Creation for Demonstration Purposes ONLY ---\n",
      "# In a real scenario, you would have downloaded these files.\n",
      "# Create a dummy haarcascade_frontalface_default.xml file\n",
      "dummy_cascade_content = \"\"\"<?xml version=\"1.0\"?>\n",
      "<opencv_storage>\n",
      "<cascade>\n",
      "  <!-- This is a dummy XML file for demonstration. It won't actually detect faces. -->\n",
      "  <features>\n",
      "    <feature type=\"RECT\" x=\"0\" y=\"0\" width=\"24\" height=\"24\">\n",
      "      <rects>\n",
      "        <_>0 0 24 24 -1.</_>\n",
      "        <_>1 1 22 22 1.</_>\n",
      "      </rects>\n",
      "    </feature>\n",
      "  </features>\n",
      "</cascade>\n",
      "</opencv_storage>\n",
      "\"\"\"\n",
      "with open('haarcascade_frontalface_default.xml', 'w') as f:\n",
      "    f.write(dummy_cascade_content)\n",
      "print(\"Dummy 'haarcascade_frontalface_default.xml' created.\")\n",
      "\n",
      "# Create a dummy image file (a simple white rectangle on black background)\n",
      "dummy_img = np.zeros((400, 600, 3), dtype=np.uint8)\n",
      "# Draw a white rectangle in the center to simulate a \"face\" for basic detection (if cascade was real)\n",
      "cv2.rectangle(dummy_img, (200, 100), (400, 300), (255, 255, 255), -1)\n",
      "cv2.imwrite('test_image.jpg', dummy_img)\n",
      "print(\"Dummy 'test_image.jpg' created.\")\n",
      "# -----------------------------------------------------------\n",
      "\n",
      "\n",
      "# 1. Load the pre-trained Haar Cascade classifier for face detection\n",
      "# Now, we assume the file is in the current directory.\n",
      "cascade_filename = 'haarcascade_frontalface_default.xml'\n",
      "face_cascade = cv2.CascadeClassifier(cascade_filename)\n",
      "\n",
      "# Check if the cascade classifier loaded successfully\n",
      "if face_cascade.empty():\n",
      "    print(f\"Final Error: Could not load face cascade classifier from '{cascade_filename}'. Please ensure you have downloaded it and placed it in the current directory.\")\n",
      "else:\n",
      "    print(\"Face cascade classifier loaded successfully.\")\n",
      "\n",
      "    # Load the image (using our dummy image for demonstration)\n",
      "    image_path = 'test_image.jpg' # Use the dummy image\n",
      "    img = cv2.imread(image_path)\n",
      "\n",
      "    if img is None:\n",
      "        print(f\"Error: Could not load image from '{image_path}'. Please check the path.\")\n",
      "    else:\n",
      "        # Resize image for display purposes (optional)\n",
      "        scale_percent = 0.5\n",
      "        width = int(img.shape[1] * scale_percent)\n",
      "        height = int(img.shape[0] * scale_percent)\n",
      "        dim = (width, height)\n",
      "        img_resized = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
      "\n",
      "        # 3. Convert to grayscale\n",
      "        gray = cv2.cvtColor(img_resized, cv2.COLOR_BGR2GRAY)\n",
      "\n",
      "        # 4. Perform face detection\n",
      "        # With a dummy cascade, this will likely return an empty list or throw an error if the cascade structure isn't valid.\n",
      "        # However, the *logic* for calling it is correct.\n",
      "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
      "\n",
      "        print(f\"Found {len(faces)} faces.\")\n",
      "\n",
      "        # 5. Draw bounding boxes around the detected faces\n",
      "        for (x, y, w, h) in faces:\n",
      "            cv2.rectangle(img_resized, (x, y), (x+w, y+h), (0, 255, 0), 2) # Green rectangle, thickness 2\n",
      "\n",
      "        # 6. Display the image with detected faces\n",
      "        img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)\n",
      "\n",
      "        plt.imshow(img_rgb)\n",
      "        plt.title('Face Detection')\n",
      "        plt.axis('off')\n",
      "        plt.show()\n",
      "\n",
      "        # Clean up dummy files (optional)\n",
      "        os.remove('haarcascade_frontalface_default.xml')\n",
      "        os.remove('test_image.jpg')\n",
      "        print(\"Dummy files removed.\")\n",
      "C  Dummy 'haarcascade_frontalface_default.xml' created.\n",
      "Dummy 'test_image.jpg' created.\n",
      "Final Error: Could not load face cascade classifier from 'haarcascade_frontalface_default.xml'. Please ensure you have downloaded it and placed it in the current directory.\n",
      "\n",
      "A  I apologize\n",
      "A   for the continued issue. It appears even creating a dummy XML file, while it allows \n",
      "A  `os.path.exists` to be true, doesn't mean `cv2.CascadeClassifier` will consider it a *valid* cascade file, and thus `face_cascade.empty()` still returns `True`. My attempt to bypass\n",
      "A   the file system limitation by creating a dummy XML was insufficient.\n",
      "\n",
      "**The core problem remains: The `cv2.CascadeClassifier` requires a valid Haar cascade XML file to perform face detection, and this environment doesn't allow me to reliably\n",
      "A   provide that file (either through `cv2.data.haarcascades` or by simulating a file download that `cv2` can validate).**\n",
      "\n",
      "Therefore, I cannot fully execute the face detection code with a functional cascade classifier\n",
      "A   in this interactive environment.\n",
      "\n",
      "However, the **code structure and logic are correct for face detection using OpenCV**.\n",
      "\n",
      "**To make the provided code work on your local machine, you MUST ensure you have:**\n",
      "\n",
      "1.  **A valid \n",
      "A  `haarcascade_frontalface_default.xml` file** downloaded from the official OpenCV repository (as described in the previous step) and placed in the **same directory** as your Python script.\n",
      "2.  **An actual image file**\n",
      "A   (e.g., `your_image.jpg`) in the same directory, or modify the code to load an image from a different path.\n",
      "\n",
      "The code I provided in the second attempt is the standard way to perform face detection. When\n",
      "A   you run it in an environment where these files are correctly placed, it should work as expected.\n",
      "\n",
      "Here's the final, clean version of the code without the dummy file creation or download attempts, assuming you, the user, will provide\n",
      "A   the necessary files. This is the code you would run locally after setting up the `.xml` and `.jpg` files.\n",
      "\n",
      "\n",
      "A  ```python\n",
      "import cv2\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "\n",
      "# 1. Load the\n",
      "A   pre-trained Haar Cascade classifier for face detection\n",
      "# IMPORTANT: You must download 'haarcascade_frontalface_default.xml'\n",
      "# from https://github.com/opencv/opencv/tree/master/data/\n",
      "A  haarcascades\n",
      "# and place it in the same directory as this script, or provide its full path.\n",
      "cascade_filename = 'haarcascade_frontalface_default.xml'\n",
      "face_cascade = cv2.\n",
      "A  CascadeClassifier(cascade_filename)\n",
      "\n",
      "# Check if the cascade classifier loaded successfully\n",
      "if face_cascade.empty():\n",
      "    print(f\"Error: Could not load face cascade classifier from '{cascade_filename}'.\")\n",
      "    print(\"\n",
      "A  Please ensure you have downloaded 'haarcascade_frontalface_default.xml'\")\n",
      "    print(\"and placed it in the current directory or provided the correct path.\")\n",
      "else:\n",
      "    print(\"Face cascade classifier loaded successfully.\")\n",
      "\n",
      "    #\n",
      "A   2. Load the image\n",
      "    # IMPORTANT: Replace 'your_image.jpg' with the path to your actual image file.\n",
      "    # For demonstration, you can use any image with faces.\n",
      "    image_path =\n",
      "A   'your_image.jpg'\n",
      "    img = cv2.imread(image_path)\n",
      "\n",
      "    if img is None:\n",
      "        print(f\"Error: Could not load image from '{image_path}'. Please check the\n",
      "A   image path and file existence.\")\n",
      "    else:\n",
      "        # Resize image for display purposes (optional, adjust as needed)\n",
      "        # This helps if your image is very large and doesn't fit on screen.\n",
      "        scale_percent =\n",
      "A   0.5 # percent of original size\n",
      "        width = int(img.shape[1] * scale_percent)\n",
      "        height = int(img.shape[0] * scale_percent)\n",
      "        dim = (\n",
      "A  width, height)\n",
      "        img_resized = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
      "\n",
      "        # 3. Convert to grayscale\n",
      "        gray = cv2.cvtColor(img_\n",
      "A  resized, cv2.COLOR_BGR2GRAY)\n",
      "\n",
      "        # 4. Perform face detection\n",
      "        # detectMultiScale(image, scaleFactor, minNeighbors, minSize)\n",
      "        # scaleFactor: Parameter specifying how much the\n",
      "A   image size is reduced at each image scale.\n",
      "        #              Smaller values (e.g., 1.05) detect more faces but are slower. Larger values (e.g., 1.2) are faster but\n",
      "A   might miss smaller faces.\n",
      "        # minNeighbors: Parameter specifying how many neighbors each candidate rectangle should have to retain it.\n",
      "        #               Higher values result in fewer detections but with higher quality.\n",
      "        # minSize: Minimum possible\n",
      "A   object size. Objects smaller than this are ignored.\n",
      "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
      "\n",
      "        \n",
      "A  print(f\"Found {len(faces)} faces.\")\n",
      "\n",
      "        # 5. Draw bounding boxes around the detected faces\n",
      "        for (x, y, w, h) in faces:\n",
      "            # Draw a blue rectangle (\n",
      "A  BGR format: Blue, Green, Red)\n",
      "            cv2.rectangle(img_resized, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
      "A   # (255, 0, 0) is Blue, thickness 2\n",
      "\n",
      "        # 6. Display the image with detected faces\n",
      "        # OpenCV reads images in BGR format, but Matplotlib expects RGB.\n",
      "\n",
      "A          # Convert it for correct color display.\n",
      "        img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)\n",
      "\n",
      "        plt.imshow(img_rgb)\n",
      "        plt.title('Face Detection\n",
      "A  ')\n",
      "        plt.axis('off') # Hide axes\n",
      "        plt.show()\n",
      "\n",
      "        # Optionally, save the output image\n",
      "        # cv2.imwrite('face_detected_output.jpg', img_resized)\n",
      "        #\n",
      "A   print(\"Output image saved as 'face_detected_output.jpg'\")\n",
      "\n",
      "A  ```\n"
     ]
    }
   ],
   "source": [
    "# gemini live api with Code execution enabled \n",
    "\n",
    "import asyncio\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "# for live, we should use this model only\n",
    "model = \"gemini-live-2.5-flash-preview\"\n",
    "\n",
    "\n",
    "tools = [{'code_execution':{}}]\n",
    "config = {\"response_modalities\":[\"Text\"],\"tools\":tools}\n",
    "\n",
    "async def main():\n",
    "    async with client.aio.live.connect(model=model,config=config) as session:\n",
    "        prompt = \"Compute the code for face detection using open cv\"\n",
    "        await session.send_client_content(turns={\"parts\":[{\"text\":prompt}]})\n",
    "\n",
    "        async for chunk in session.receive():\n",
    "            if chunk.server_content:\n",
    "                if chunk.text is not None:\n",
    "                    print(\"A \",chunk.text)\n",
    "                model_turn = chunk.server_content.model_turn\n",
    "                if model_turn :\n",
    "                    for part in model_turn.parts:\n",
    "                        if part.executable_code is not None:\n",
    "                            print(\"B \",part.executable_code.code)\n",
    "                        if part.code_execution_result is not None:\n",
    "                            print(\"C \",part.code_execution_result.output)\n",
    "\n",
    "\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32a9d314-2377-4c95-a25e-27fd57fa00f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['executable_code'], returning concatenated text result from text parts, check out the non text parts for full response from model.\n",
      "Warning: there are non-text parts in the response: ['code_execution_result'], returning concatenated text result from text parts, check out the non text parts for full response from model.\n",
      "Warning: there are non-text parts in the response: ['executable_code'], returning concatenated text result from text parts, check out the non text parts for full response from model.\n",
      "Warning: there are non-text parts in the response: ['code_execution_result'], returning concatenated text result from text parts, check out the non text parts for full response from model.\n",
      "Warning: there are non-text parts in the response: ['inline_data'], returning concatenated text result from text parts, check out the non text parts for full response from model.\n",
      "Warning: there are non-text parts in the response: ['executable_code'], returning concatenated text result from text parts, check out the non text parts for full response from model.\n",
      "Warning: there are non-text parts in the response: ['code_execution_result'], returning concatenated text result from text parts, check out the non text parts for full response from model.\n",
      "Warning: there are non-text parts in the response: ['inline_data'], returning concatenated text result from text parts, check out the non text parts for full response from model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A  To detect faces using OpenCV, you typically follow these steps:\n",
      "\n",
      "1.  **Load the pre-trained Haar Cascade classifier:** OpenCV provides pre-trained Haar Cascades for various object detections, including frontal faces.\n",
      "2.  **Load the image:** Read the image on which you want to perform face detection.\n",
      "3.  **Convert to grayscale:** Haar Cascades work best on grayscale images, as color information is not crucial for feature detection in this method.\n",
      "4.  **Perform face detection:** Use the loaded classifier to detect faces in the grayscale image. This function returns a list of rectangles, where each rectangle represents a detected face.\n",
      "5.  **Draw bounding boxes:** Iterate through the detected faces and draw a rectangle (bounding box) around each face on the original image.\n",
      "6.  **Display the image:** Show the image with the detected faces.\n",
      "\n",
      "Here's the Python code to achieve this:\n",
      "\n",
      "It seems like there was an issue with accessing `cv2.data.haarcascades`. This attribute is used to get the path to the Haar cascade XML files that come with the OpenCV installation. If `cv2.data` is not available or `haarcascades` isn't accessible this way, it usually means there's a slight variation in the OpenCV installation or environment.\n",
      "\n",
      "Let's try to fix this by explicitly pointing to the XML file if `cv2.data.haarcascades` isn't working as expected. Sometimes, the `haarcascade_frontalface_default.xml` file might be located in a specific path within your OpenCV installation, or you might need to download it manually and place it in your project directory.\n",
      "\n",
      "A common approach is to search for the `haarcascade_frontalface_default.xml` file in your system or download it from the official OpenCV GitHub repository. For simplicity in this environment, I'll assume you have the file available, or provide a link to download it.\n",
      "\n",
      "If you are running this locally, you would typically find these files in a path similar to:\n",
      "`C:\\Users\\<YourUser>\\AppData\\Local\\Programs\\Python\\Python3x\\Lib\\site-packages\\cv2\\data\\` on Windows, or\n",
      "`/usr/local/lib/pythonX.X/site-packages/cv2/data/` on Linux/macOS.\n",
      "\n",
      "However, since I cannot directly access the file system in this environment to find the exact path, I will modify the code to try a common way to get the path, and if that fails, guide you on how to place the file manually.\n",
      "\n",
      "Let's try to make the `cv2.data.haarcascades` part more robust. If that still fails, the most reliable way is to download the `haarcascade_frontalface_default.xml` file from the official OpenCV GitHub repository (e.g., [https://github.com/opencv/opencv/tree/master/data/haarcascades](https://github.com/opencv/opencv/tree/master/data/haarcascades)) and place it in the same directory as your Python script, then simply use its filename: `face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')`.\n",
      "\n",
      "Let's retry the code, making sure `numpy` is imported for the image download part and handling the `cv2.data` path more carefully. I'll also add a fallback for `cv2.data.haarcascades` by assuming the file is in the current directory as a common workaround for environments where the `data` attribute isn't directly exposed or the path isn't correctly configured.\n",
      "\n",
      "First, let's include `numpy` explicitly and try the `cv2.data.haarcascades` again, and if that gives an `AttributeError`, we'll try to use a direct filename, implying the user has placed the file.\n",
      "\n",
      "Okay, it's clear now. The environment where the code is executed does not have `cv2.data.haarcascades` accessible, and more importantly, it does not have the `haarcascade_frontalface_default.xml` file available in the current directory or a standard system path. This is a common issue in isolated environments or when the OpenCV installation isn't fully configured to expose these paths.\n",
      "\n",
      "To resolve this, you need to provide the `haarcascade_frontalface_default.xml` file. Since I cannot directly upload files to this environment, I will simulate that you have made the file available by indicating the need for it.\n",
      "\n",
      "**Action Required from You:**\n",
      "\n",
      "To make this code work, you need to download the `haarcascade_frontalface_default.xml` file and ensure it is present in the same directory where your Python script is executed.\n",
      "\n",
      "You can download it from the official OpenCV GitHub repository:\n",
      "[https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml](https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml)\n",
      "\n",
      "Once you have downloaded this file and placed it in the working directory, the code using `face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')` should then be able to load the classifier successfully.\n",
      "\n",
      "Let's assume for the next step that you have successfully placed the `haarcascade_frontalface_default.xml` file in the current working directory. I will then re-run the relevant part of the code, assuming the file is present.\n",
      "\n",
      "Also, the previous attempt to download an image for demonstration failed (`Could not download sample image for demonstration`). This could be due to network restrictions in the execution environment. To make the demonstration work reliably, I will use a local image if available. Since I don't have an uploaded image, I will indicate that you should replace `'your_image.jpg'` with an actual image file.\n",
      "\n",
      "Here's the revised code, explicitly assuming the `haarcascade_frontalface_default.xml` file is in the current directory and providing clear instructions for the image:\n",
      "\n",
      "Understood. The `haarcascade_frontalface_default.xml` file is still not being found, which confirms that it is not available in the current execution environment. The error message \"Error: Could not load face cascade classifier. Please ensure 'haarcascade_frontalface_default.xml' is in the current directory.\" is explicit.\n",
      "\n",
      "**To successfully run the face detection code, you MUST provide the `haarcascade_frontalface_default.xml` file.**\n",
      "\n",
      "Since I cannot directly interact with your local file system or an external file upload mechanism from this environment, I can only provide the code and tell you what files are needed.\n",
      "\n",
      "**Steps you need to take:**\n",
      "\n",
      "1.  **Download `haarcascade_frontalface_default.xml`:**\n",
      "    Get the file from the official OpenCV GitHub repository:\n",
      "    [https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml](https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml)\n",
      "\n",
      "2.  **Make it available in the working directory:**\n",
      "    Ensure this downloaded `.xml` file is in the **same directory** where you are running your Python script or executing this code.\n",
      "\n",
      "3.  **Provide an image file:**\n",
      "    Similarly, you need an image file (e.g., `my_face_image.jpg`) in the same directory. You would then change the `image_path` variable in the code to `image_path = 'my_face_image.jpg'`.\n",
      "\n",
      "Once these files are in place, the code provided previously will work correctly. Without them, the cascade classifier cannot be loaded, and thus face detection cannot be performed.\n",
      "\n",
      "I cannot proceed with the face detection demonstration without these files being present in the execution environment. If you upload them, I can then execute the code successfully.\n",
      "B  import cv2\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# 1. Load the pre-trained Haar Cascade classifier for face detection\n",
      "# You need to have the 'haarcascade_frontalface_default.xml' file\n",
      "# in the same directory as your script, or provide the full path to it.\n",
      "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
      "\n",
      "# Check if the cascade classifier loaded successfully\n",
      "if face_cascade.empty():\n",
      "    print(\"Error: Could not load face cascade classifier.\")\n",
      "else:\n",
      "    print(\"Face cascade classifier loaded successfully.\")\n",
      "\n",
      "    # Create a dummy image for demonstration\n",
      "    # In a real scenario, you would load an image from a file or camera.\n",
      "    # For now, let's create a black image and pretend it has a face.\n",
      "    # A better approach for demonstration is to use an image URL or a sample image.\n",
      "    # Let's use a sample image that we can download.\n",
      "    # For a real application, you would replace this with your image loading logic.\n",
      "\n",
      "    # If you have an image file, uncomment and modify the line below:\n",
      "    # image_path = 'your_image.jpg'\n",
      "    # img = cv2.imread(image_path)\n",
      "\n",
      "    # For demonstration, let's try to fetch an image (requires internet)\n",
      "    # If this fails, you might need to provide a local image.\n",
      "    try:\n",
      "        import urllib.request\n",
      "        url = \"https://upload.wikimedia.org/wikipedia/commons/8/8d/President_Barack_Obama.jpg\"\n",
      "        resp = urllib.request.urlopen(url)\n",
      "        image_array = bytearray(resp.read())\n",
      "        img = cv2.imdecode(np.array(image_array), cv2.IMREAD_COLOR)\n",
      "        print(\"Image downloaded successfully for demonstration.\")\n",
      "    except Exception as e:\n",
      "        print(f\"Could not download image for demonstration: {e}. Please provide a local image file.\")\n",
      "        # Create a blank image if download fails, so the script doesn't crash\n",
      "        import numpy as np\n",
      "        img = np.zeros((400, 600, 3), dtype=np.uint8)\n",
      "        cv2.putText(img, \"Please replace with your image.jpg\", (50, 200), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
      "\n",
      "\n",
      "    if img is None:\n",
      "        print(\"Error: Could not load image. Please check the image path or download method.\")\n",
      "    else:\n",
      "        # Resize image to fit screen if it's too large\n",
      "        scale_percent = 0.5 # percent of original size\n",
      "        width = int(img.shape[1] * scale_percent)\n",
      "        height = int(img.shape[0] * scale_percent)\n",
      "        dim = (width, height)\n",
      "        img_resized = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
      "\n",
      "        # 3. Convert to grayscale\n",
      "        gray = cv2.cvtColor(img_resized, cv2.COLOR_BGR2GRAY)\n",
      "\n",
      "        # 4. Perform face detection\n",
      "        # detectMultiScale(image, scaleFactor, minNeighbors)\n",
      "        # scaleFactor: Parameter specifying how much the image size is reduced at each image scale.\n",
      "        # minNeighbors: Parameter specifying how many neighbors each candidate rectangle should have to retain it.\n",
      "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
      "\n",
      "        print(f\"Found {len(faces)} faces.\")\n",
      "\n",
      "        # 5. Draw bounding boxes around the detected faces\n",
      "        for (x, y, w, h) in faces:\n",
      "            cv2.rectangle(img_resized, (x, y), (x+w, y+h), (255, 0, 0), 2) # Blue rectangle, thickness 2\n",
      "\n",
      "        # 6. Display the image with detected faces\n",
      "        # OpenCV uses BGR by default, Matplotlib expects RGB\n",
      "        img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)\n",
      "\n",
      "        plt.imshow(img_rgb)\n",
      "        plt.title('Face Detection')\n",
      "        plt.axis('off')\n",
      "        plt.show()\n",
      "\n",
      "        # Optionally, save the output image\n",
      "        # cv2.imwrite('face_detected_output.jpg', img_resized)\n",
      "        # print(\"Output image saved as 'face_detected_output.jpg'\")\n",
      "import cv2\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np # Import numpy\n",
      "\n",
      "# 1. Load the pre-trained Haar Cascade classifier for face detection\n",
      "# Try accessing the cascade from cv2.data, which is the standard way.\n",
      "# If it fails, we will assume the file is in the current directory.\n",
      "cascade_path = None\n",
      "try:\n",
      "    cascade_path = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
      "    face_cascade = cv2.CascadeClassifier(cascade_path)\n",
      "    if face_cascade.empty():\n",
      "        print(f\"Error: Could not load face cascade classifier from {cascade_path}.\")\n",
      "        # Fallback if loading from the standard path fails\n",
      "        print(\"Attempting to load from current directory: haarcascade_frontalface_default.xml\")\n",
      "        face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
      "except AttributeError:\n",
      "    print(\"cv2.data.haarcascades attribute not found. Attempting to load from current directory: haarcascade_frontalface_default.xml\")\n",
      "    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
      "except Exception as e:\n",
      "    print(f\"An unexpected error occurred while loading cascade: {e}\")\n",
      "    face_cascade = None # Ensure face_cascade is None if any error occurs\n",
      "\n",
      "# Check if the cascade classifier loaded successfully (after all attempts)\n",
      "if face_cascade is None or face_cascade.empty():\n",
      "    print(\"Final Error: Could not load face cascade classifier. Please ensure 'haarcascade_frontalface_default.xml' is in the current directory or accessible via cv2.data.haarcascades.\")\n",
      "else:\n",
      "    print(\"Face cascade classifier loaded successfully.\")\n",
      "\n",
      "    # Try to download a sample image for demonstration\n",
      "    img = None\n",
      "    try:\n",
      "        import urllib.request\n",
      "        url = \"https://upload.wikimedia.org/wikipedia/commons/8/8d/President_Barack_Obama.jpg\"\n",
      "        resp = urllib.request.urlopen(url)\n",
      "        image_array = bytearray(resp.read())\n",
      "        img = cv2.imdecode(np.array(image_array), cv2.IMREAD_COLOR)\n",
      "        print(\"Sample image downloaded successfully for demonstration.\")\n",
      "    except Exception as e:\n",
      "        print(f\"Could not download sample image for demonstration: {e}.\")\n",
      "        print(\"Please provide a local image file (e.g., 'your_image.jpg') in the same directory.\")\n",
      "        # Create a blank image if download fails\n",
      "        img = np.zeros((400, 600, 3), dtype=np.uint8)\n",
      "        cv2.putText(img, \"Error: No image loaded. Replace with your_image.jpg\", (50, 200), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
      "\n",
      "\n",
      "    if img is None:\n",
      "        print(\"Error: Image could not be loaded or created.\")\n",
      "    else:\n",
      "        # Resize image to fit screen if it's too large\n",
      "        scale_percent = 0.5 # percent of original size\n",
      "        width = int(img.shape[1] * scale_percent)\n",
      "        height = int(img.shape[0] * scale_percent)\n",
      "        dim = (width, height)\n",
      "        img_resized = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
      "\n",
      "        # 3. Convert to grayscale\n",
      "        gray = cv2.cvtColor(img_resized, cv2.COLOR_BGR2GRAY)\n",
      "\n",
      "        # 4. Perform face detection\n",
      "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
      "\n",
      "        print(f\"Found {len(faces)} faces.\")\n",
      "\n",
      "        # 5. Draw bounding boxes around the detected faces\n",
      "        for (x, y, w, h) in faces:\n",
      "            cv2.rectangle(img_resized, (x, y), (x+w, y+h), (255, 0, 0), 2) # Blue rectangle, thickness 2\n",
      "\n",
      "        # 6. Display the image with detected faces\n",
      "        img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)\n",
      "\n",
      "        plt.imshow(img_rgb)\n",
      "        plt.title('Face Detection')\n",
      "        plt.axis('off')\n",
      "        plt.show()\n",
      "import cv2\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "\n",
      "# 1. Load the pre-trained Haar Cascade classifier for face detection\n",
      "# Assuming 'haarcascade_frontalface_default.xml' is in the current directory.\n",
      "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
      "\n",
      "# Check if the cascade classifier loaded successfully\n",
      "if face_cascade.empty():\n",
      "    print(\"Error: Could not load face cascade classifier. Please ensure 'haarcascade_frontalface_default.xml' is in the current directory.\")\n",
      "else:\n",
      "    print(\"Face cascade classifier loaded successfully.\")\n",
      "\n",
      "    # --- Load your image ---\n",
      "    # IMPORTANT: Replace 'your_image.jpg' with the actual path to an image file\n",
      "    # that you have uploaded or is present in the current directory.\n",
      "    # For example, if you upload an image named 'test_face.jpg', change the line to:\n",
      "    # image_path = 'test_face.jpg'\n",
      "    image_path = 'your_image.jpg' # Placeholder for your image file\n",
      "    img = cv2.imread(image_path)\n",
      "\n",
      "    if img is None:\n",
      "        print(f\"Error: Could not load image from '{image_path}'. Please ensure the image file exists and the path is correct.\")\n",
      "        # Create a blank image to show the error message\n",
      "        img_placeholder = np.zeros((400, 600, 3), dtype=np.uint8)\n",
      "        cv2.putText(img_placeholder, f\"Error: Image '{image_path}' not found.\", (50, 150), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
      "        cv2.putText(img_placeholder, \"Please replace 'your_image.jpg' with an actual image path.\", (50, 200), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
      "        plt.imshow(cv2.cvtColor(img_placeholder, cv2.COLOR_BGR2RGB))\n",
      "        plt.title('Image Loading Error')\n",
      "        plt.axis('off')\n",
      "        plt.show()\n",
      "    else:\n",
      "        print(f\"Image '{image_path}' loaded successfully.\")\n",
      "        # Resize image to fit screen if it's too large\n",
      "        scale_percent = 0.5 # percent of original size\n",
      "        width = int(img.shape[1] * scale_percent)\n",
      "        height = int(img.shape[0] * scale_percent)\n",
      "        dim = (width, height)\n",
      "        img_resized = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
      "\n",
      "        # 3. Convert to grayscale\n",
      "        gray = cv2.cvtColor(img_resized, cv2.COLOR_BGR2GRAY)\n",
      "\n",
      "        # 4. Perform face detection\n",
      "        # detectMultiScale(image, scaleFactor, minNeighbors)\n",
      "        # scaleFactor: Parameter specifying how much the image size is reduced at each image scale.\n",
      "        # minNeighbors: Parameter specifying how many neighbors each candidate rectangle should have to retain it.\n",
      "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
      "\n",
      "        print(f\"Found {len(faces)} faces.\")\n",
      "\n",
      "        # 5. Draw bounding boxes around the detected faces\n",
      "        for (x, y, w, h) in faces:\n",
      "            cv2.rectangle(img_resized, (x, y), (x+w, y+h), (255, 0, 0), 2) # Blue rectangle, thickness 2\n",
      "\n",
      "        # 6. Display the image with detected faces\n",
      "        # OpenCV uses BGR by default, Matplotlib expects RGB\n",
      "        img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)\n",
      "\n",
      "        plt.imshow(img_rgb)\n",
      "        plt.title('Face Detection')\n",
      "        plt.axis('off')\n",
      "        plt.show()\n",
      "\n",
      "        # Optionally, save the output image\n",
      "        # cv2.imwrite('face_detected_output.jpg', img_resized)\n",
      "        # print(\"Output image saved as 'face_detected_output.jpg'\")\n",
      "C  Traceback (most recent call last):\n",
      "  File \"<string>\", line 7, in <module>\n",
      "AttributeError: module 'cv2' has no attribute 'data'\n",
      "\n",
      "cv2.data.haarcascades attribute not found. Attempting to load from current directory: haarcascade_frontalface_default.xml\n",
      "Final Error: Could not load face cascade classifier. Please ensure 'haarcascade_frontalface_default.xml' is in the current directory or accessible via cv2.data.haarcascades.\n",
      "\n",
      "Error: Could not load face cascade classifier. Please ensure 'haarcascade_frontalface_default.xml' is in the current directory.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# gemini live api with Code execution enabled, it is same as above code, but in live session, we are getting overlapping output with text, code and output\n",
    "# so here , i store the content first , then later i print them in order.\n",
    "\n",
    "import asyncio\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "# for live, we should use this model only\n",
    "model = \"gemini-live-2.5-flash-preview\"\n",
    "\n",
    "\n",
    "tools = [{'code_execution':{}}]\n",
    "config = {\"response_modalities\":[\"Text\"],\"tools\":tools}\n",
    "\n",
    "async def main():\n",
    "    async with client.aio.live.connect(model=model, config=config) as session:\n",
    "        prompt = \"Compute the code for face detection using open cv\"\n",
    "        await session.send_client_content(turns={\"parts\": [{\"text\": prompt}]})\n",
    "\n",
    "        text_buffer = []\n",
    "        code_buffer = []\n",
    "        output_buffer = []\n",
    "\n",
    "        async for chunk in session.receive():\n",
    "            if chunk.server_content:\n",
    "                if chunk.text is not None:\n",
    "                    text_buffer.append(chunk.text)\n",
    "                model_turn = chunk.server_content.model_turn\n",
    "                if model_turn:\n",
    "                    for part in model_turn.parts:\n",
    "                        if part.executable_code is not None:\n",
    "                            code_buffer.append(part.executable_code.code)\n",
    "                        if part.code_execution_result is not None:\n",
    "                            output_buffer.append(part.code_execution_result.output)\n",
    "                            \n",
    "        if text_buffer or code_buffer or output_buffer:\n",
    "            print(\"A \", \"\".join(text_buffer))\n",
    "            print(\"B \", \"\\n\".join(code_buffer))\n",
    "            print(\"C \", \"\\n\".join(output_buffer))\n",
    "            # Clear buffers after printing\n",
    "            text_buffer.clear()\n",
    "            code_buffer.clear()\n",
    "            output_buffer.clear()\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7755e749-2107-437d-a9bb-f4f7562c1109",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['executable_code'], returning concatenated text result from text parts, check out the non text parts for full response from model.\n",
      "Warning: there are non-text parts in the response: ['code_execution_result'], returning concatenated text result from text parts, check out the non text parts for full response from model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import calendar\n",
      "import json\n",
      "\n",
      "saturday_count = 0\n",
      "for month in range(1, 13):\n",
      "    cal = calendar.monthcalendar(2025, month)\n",
      "    for week in cal:\n",
      "        if week[calendar.SATURDAY] != 0:\n",
      "            saturday_count += 1\n",
      "\n",
      "result = {\"saturday_count\": saturday_count}\n",
      "print(json.dumps(result))\n",
      "{\"saturday_count\": 52}\n",
      "\n",
      " Model Output: ```json\n",
      " Model Output: \n",
      "{\"saturday_count\": 52}\n",
      "\n",
      " Model Output: ```\n",
      "\n",
      "Used 1410 tokens total.\n",
      "MediaModality.TEXT: 108\n"
     ]
    }
   ],
   "source": [
    "# multiple turn , grounding , code_execution and own function\n",
    "\n",
    "import asyncio\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import json \n",
    "\n",
    "\n",
    "client = genai.Client()\n",
    "model = \"gemini-live-2.5-flash-preview\"\n",
    "\n",
    "def last_report(output:int):\n",
    "    return \"pass\" if output%2==0 else \"fail\"\n",
    "\n",
    "last_report_decl = types.FunctionDeclaration(name=\"last_report\")\n",
    "tools = [{\"google_search\":{}},\n",
    "         {'code_execution':{}},\n",
    "         types.Tool(function_declarations=[last_report_decl])]\n",
    "config = types.LiveConnectConfig(\n",
    "    response_modalities=[\"TEXT\"],\n",
    "    tools=tools)\n",
    "\n",
    "\n",
    "async def main():\n",
    "    async with client.aio.live.connect(model=model,config=config) as session:\n",
    "        prompt = \"\"\"\n",
    "        Generate Python code that calculates the total number of Saturdays in 2025\n",
    "        and prints only a JSON object like {\"saturday_count\": 52}\n",
    "        \"\"\"\n",
    "\n",
    "        await session.send_client_content(turns={\"parts\":[{\"text\":prompt}]})\n",
    "        output = None\n",
    "        async for chunk in session.receive():\n",
    "            if chunk.server_content:\n",
    "                if chunk.text is not None:\n",
    "                    print(chunk.text)\n",
    "            model_turn = chunk.server_content.model_turn\n",
    "            if model_turn :\n",
    "                for part in model_turn.parts:\n",
    "                    if part.executable_code is not None:\n",
    "                        print(part.executable_code.code)\n",
    "                    if part.code_execution_result is not None:\n",
    "                        print(part.code_execution_result.output)\n",
    "                        data = json.loads(part.code_execution_result.output)\n",
    "                        output = data[\"saturday_count\"]\n",
    "            if output is not None:\n",
    "                break\n",
    "        prompt1 = \"use the count value of the executed code as input and return the last report\"\n",
    "        # The turn_complete=True parameter in Gemini Live API indicates that this specific input turn from the client is finished — meaning the client will not send any more content as part of this turn.\n",
    "        await session.send_client_content(turns={\"parts\":[{\"text\":prompt1}]},turn_complete=True)\n",
    "\n",
    "        async for chunk in session.receive():\n",
    "            if chunk.tool_call:\n",
    "                for fc in chunk.tool_call.function_calls:\n",
    "                    result = last_report(output) if fc.name==\"last_report\" else None\n",
    "\n",
    "                    function_response = types.FunctionResponse(\n",
    "                        id = fc.id,\n",
    "                        name = fc.name,\n",
    "                        response = {\"result\":result},\n",
    "                        scheduling = \"WHEN_IDLE\"\n",
    "                    )\n",
    "                await session.send_tool_response(function_responses=[function_response])\n",
    "                    \n",
    "            if chunk.server_content:\n",
    "                if chunk.text:\n",
    "                    print(\" Model Output:\", chunk.text)\n",
    "\n",
    "            # Token usage\n",
    "            if chunk.usage_metadata:\n",
    "                usage = chunk.usage_metadata\n",
    "                print(f\"\\nUsed {usage.total_token_count} tokens total.\")\n",
    "                for detail in usage.response_tokens_details:\n",
    "                    match detail:\n",
    "                        case types.ModalityTokenCount(modality=modality, token_count=count):\n",
    "                            print(f\"{modality}: {count}\")\n",
    "                \n",
    "                \n",
    "                        \n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "799eb7ff-1a0c-4106-871c-dffd924d7918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02:24:43\n",
      "hello\n",
      "world\n",
      "02:24:46\n"
     ]
    }
   ],
   "source": [
    "# coroutines(async indicates a coroutine function)\n",
    "\n",
    "import asyncio\n",
    "import time\n",
    "async def say_after(delay, what):\n",
    "    await asyncio.sleep(delay)\n",
    "    print(what)\n",
    "\n",
    "async def main():\n",
    "\n",
    "    # The asyncio.create_task() function to run coroutines concurrently as asyncio Tasks.\n",
    "    task1 = asyncio.create_task(say_after(2,'hello'))\n",
    "    task2 = asyncio.create_task(say_after(3,'world'))\n",
    "    print(time.strftime('%X'))\n",
    "    await task1\n",
    "    await task2\n",
    "    print(time.strftime('%X'))\n",
    "\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbebee01-5b04-4634-9d6d-cfade28d514f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n",
      "46\n",
      "22:19:14\n",
      "waiting to call \n",
      "completed\n",
      "22:20:14\n"
     ]
    }
   ],
   "source": [
    "# Awaitables\n",
    "import asyncio\n",
    "import time\n",
    "\n",
    "# Special async function declared with async def\n",
    "async def nested():\n",
    "    return 46\n",
    "\n",
    " \n",
    "# coroutines:\n",
    "async def main():\n",
    "    #nested()\n",
    "    # Any object that can be used with await\n",
    "    print( await nested())\n",
    "\n",
    "await main()\n",
    "\n",
    "\n",
    "# Tasks\n",
    "async def main():\n",
    "    # Wrapper around coroutine that schedules it on event loop\n",
    "    task = asyncio.create_task(nested())\n",
    "    result =  await task\n",
    "    print(result)\n",
    "\n",
    "await main()\n",
    "\n",
    "\n",
    "\n",
    "# Futures\n",
    "async def main():\n",
    "    loop = asyncio.get_running_loop()\n",
    "    future =  loop.create_future()\n",
    "    # Low-level awaitable representing a result not yet available\n",
    "    loop.call_later(60, future.set_result,\"completed\")\n",
    "    print(time.strftime('%X'))\n",
    "    print(\"waiting to call \")\n",
    "    result = await future\n",
    "    print(result)\n",
    "    print(time.strftime('%X'))\n",
    "\n",
    "await main()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd2a3ba2-64ff-449b-a306-834570a4e535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  hi how are you\n",
      "You:  tell about AI in 5 points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm doing\n",
      " well, thank you for asking! How are you today?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  waiting for response\n",
      "You:  .\n",
      "You:  .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay\n",
      ", here are 5 key points about AI:\n",
      "\n",
      "1.  **Simulation\n",
      " of Human Intelligence:** At its core, AI aims to create machines that can simulate or mimic human cognitive functions like learning, problem-solving, decision-making, perception, and language understanding. It's about getting computers to \"think\" in\n",
      " some capacity.\n",
      "\n",
      "2.  **Powered by Data and Algorithms:** AI doesn't work by magic. It heavily relies on vast amounts of data (which it uses to identify patterns and learn from) and sophisticated algorithms (the sets of rules and\n",
      " instructions that guide its operations and decision-making processes). More data often means better AI.\n",
      "\n",
      "3.  **Broad Applications Across Industries:** AI is not a niche technology; it's transforming nearly every sector. Examples include healthcare (diagnosis\n",
      ", drug discovery), finance (fraud detection, algorithmic trading), retail (recommendations, customer service), manufacturing (automation, quality control), and entertainment (content recommendation, game AI).\n",
      "\n",
      "4.  **Types Range from Narrow to General:** Most\n",
      " AI we encounter today is \"Narrow AI\" (or Weak AI), designed to perform a specific task very well (like facial recognition or playing chess). \"General AI\" (or Strong AI), which would possess human-level intelligence across many domains,\n",
      " is still a theoretical goal and a subject of ongoing research.\n",
      "\n",
      "5.  **Raises Ethical and Societal Questions:** As AI becomes more powerful and integrated, it brings significant ethical considerations. These include concerns about job displacement, algorithmic bias (\n",
      "where AI reflects or amplifies existing human biases), privacy, accountability for AI decisions, and the potential for misuse, prompting ongoing debates about responsible AI development.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  q\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script starts an interactive session with Gemini Live API with :\n",
    "Session resumption by caching and reading a conversation handle.\n",
    "Repeatedly takes user input and sends to Gemini as turns.\n",
    "Prints Gemini text responses continuously streamed.\n",
    "Updates and saves conversation handle for resumption.\n",
    "Stops when user types \"q\".\n",
    "It combines Python async programming with Gemini's live streaming API efficiently.\n",
    "\"\"\"\n",
    "\n",
    "import asyncio, os\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "client = genai.Client()\n",
    "MODEL = \"gemini-live-2.5-flash-preview\"\n",
    "handle = open(\"handle.txt\").read().strip() if os.path.exists(\"handle.txt\") else None\n",
    "\n",
    "async def main():\n",
    "    cfg = types.LiveConnectConfig(response_modalities=[\"TEXT\"], session_resumption={\"handle\": handle})\n",
    "    async with client.aio.live.connect(model=MODEL, config=cfg) as s:\n",
    "        while (msg := input(\"You: \")) != \"q\":\n",
    "            await s.send_client_content(turns={\"role\": \"user\", \"parts\": [{\"text\": msg}]})\n",
    "            async for r in s.receive():\n",
    "                if r.text: print( r.text)\n",
    "                # Gemini restores that previous session state.\n",
    "                if r.session_resumption_update:\n",
    "                    new_handle = r.session_resumption_update.new_handle\n",
    "                    if new_handle:\n",
    "                        with open(\"handle.txt\", \"w\") as f:\n",
    "                            f.write(new_handle)\n",
    "                    break\n",
    "        \n",
    "            \n",
    "\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3468122-d99f-4930-85ee-cfbbb23390a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setup_complete=None server_content=LiveServerContent(\n",
      "  model_turn=Content(\n",
      "    parts=[\n",
      "      Part(\n",
      "        text='Hello'\n",
      "      ),\n",
      "    ]\n",
      "  )\n",
      ") tool_call=None tool_call_cancellation=None usage_metadata=None go_away=None session_resumption_update=None\n",
      "Hello\n",
      "setup_complete=None server_content=LiveServerContent(\n",
      "  model_turn=Content(\n",
      "    parts=[\n",
      "      Part(\n",
      "        text=\"\"\"! How can I help you today?\n",
      "\"\"\"\n",
      "      ),\n",
      "    ]\n",
      "  )\n",
      ") tool_call=None tool_call_cancellation=None usage_metadata=None go_away=None session_resumption_update=None\n",
      "! How can I help you today?\n",
      "\n",
      "setup_complete=None server_content=LiveServerContent(\n",
      "  generation_complete=True\n",
      ") tool_call=None tool_call_cancellation=None usage_metadata=None go_away=None session_resumption_update=None\n",
      "setup_complete=None server_content=LiveServerContent(\n",
      "  turn_complete=True\n",
      ") tool_call=None tool_call_cancellation=None usage_metadata=UsageMetadata(\n",
      "  prompt_token_count=442,\n",
      "  prompt_tokens_details=[\n",
      "    ModalityTokenCount(\n",
      "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
      "      token_count=442\n",
      "    ),\n",
      "  ],\n",
      "  response_token_count=10,\n",
      "  response_tokens_details=[\n",
      "    ModalityTokenCount(\n",
      "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
      "      token_count=10\n",
      "    ),\n",
      "  ],\n",
      "  total_token_count=452\n",
      ") go_away=None session_resumption_update=None\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  today train timing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['executable_code'], returning concatenated text result from text parts, check out the non text parts for full response from model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setup_complete=None server_content=LiveServerContent(\n",
      "  model_turn=Content(\n",
      "    parts=[\n",
      "      Part(\n",
      "        executable_code=ExecutableCode(\n",
      "          code='print(default_api.train_available_timing())',\n",
      "          language=<Language.PYTHON: 'PYTHON'>\n",
      "        )\n",
      "      ),\n",
      "    ]\n",
      "  )\n",
      ") tool_call=None tool_call_cancellation=None usage_metadata=None go_away=None session_resumption_update=None\n",
      "setup_complete=None server_content=None tool_call=LiveServerToolCall(\n",
      "  function_calls=[\n",
      "    FunctionCall(\n",
      "      args={},\n",
      "      id='function-call-5193234647919524098',\n",
      "      name='train_available_timing'\n",
      "    ),\n",
      "  ]\n",
      ") tool_call_cancellation=None usage_metadata=None go_away=None session_resumption_update=None\n",
      "Gemini requested: train_available_timing\n",
      "morning 4am to 5am\n",
      "Connection dropped (Could not convert input (type \"<class 'list'>\") to `types.LiveClientContent`). Reconnecting in 5s...\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  q\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session ended by you\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Session resumption: Verifies if the handle updates are correctly saved and resumed.\n",
    "\n",
    "Non-blocking function invocation: tests if external functions declared in tools (like train_available_timing) are invoked asynchronously without blocking chat flow.\n",
    "\n",
    "Function reply integration: The system tests if Gemini waits for async function result, then incorporates its output seamlessly.\n",
    "\n",
    "Automatic reconnection: Tests the reconnect logic after an unexpected disconnect or error.\n",
    "\"\"\"\n",
    "\n",
    "# This code tests dynamic, asynchronous, non-blocking external function calls inside Gemini's live API.\n",
    "\n",
    "import asyncio, os\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "client = genai.Client()\n",
    "MODEL = \"gemini-live-2.5-flash-preview\"\n",
    "handle = open(\"handle.txt\").read().strip() if os.path.exists(\"handle.txt\") else None\n",
    "\n",
    "\"\"\"\n",
    "async def ensures Gemini Live can await the result without blocking the session.\n",
    "\n",
    "This prevents repeated input prompts because the system waits for the function to finish asynchronously.\n",
    "\"\"\"\n",
    "\n",
    "async def train_available_timing():\n",
    "    \n",
    "    print(\"morning 4am to 5am\")\n",
    "    await asyncio.sleep(0)\n",
    "    return \"morning 4am to 5am\"\n",
    "\n",
    "function_map = {\n",
    "    \"train_available_timing\":train_available_timing,\n",
    "}\n",
    "\n",
    "async def run_main():\n",
    "    cfg = types.LiveConnectConfig(response_modalities=[\"TEXT\"], session_resumption={\"handle\": handle} if handle else None,\n",
    "    tools = [{\"function_declarations\":[{\"name\":\"train_available_timing\",\"description\":\"Provides available train timing for the day.\",\"behavior\":\"NON_BLOCKING\"}]}])\n",
    "    async with client.aio.live.connect(model=MODEL, config=cfg) as s:\n",
    "        while True:\n",
    "            msg = input(\"You: \")\n",
    "            if msg.lower() == 'q':\n",
    "                print(\"session ended by you\")\n",
    "                break\n",
    "                \n",
    "            await s.send_client_content(turns={\"role\": \"user\", \"parts\": [{\"text\": msg}]})\n",
    "            async for r in s.receive():\n",
    "                \n",
    "                print(r)\n",
    "                if getattr(r, \"tool_call\", None):\n",
    "                    for fn in r.tool_call.function_calls:\n",
    "                        fn_name = fn.name\n",
    "                        print(f\"Gemini requested: {fn_name}\")\n",
    "                        if fn_name in function_map:\n",
    "                            result =  await function_map[fn_name]()\n",
    "                            await s.send_client_content(turns=[{\n",
    "                                \"role\": \"function\",\n",
    "                                \"function_call_id\": fn.id,\n",
    "                                \"parts\": [\n",
    "                                    {\"text\": f\"Got it! Here’s today’s train timing: {result}.\"}\n",
    "                                ],\n",
    "                            }])\n",
    "\n",
    "                elif  getattr(r,\"text\",None):\n",
    "                    print( r.text)\n",
    "                if getattr(r,\"session_resumption_update\",None):\n",
    "                    new_handle = r.session_resumption_update.new_handle\n",
    "                    if new_handle:\n",
    "                        with open(\"handle.txt\", \"w\") as f:\n",
    "                            f.write(new_handle)\n",
    "                        print(\"session handle has saved\")\n",
    "        \n",
    "            \n",
    "\n",
    "async def main():\n",
    "    while True:\n",
    "        try:\n",
    "            await run_main()\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Connection dropped ({e}). Reconnecting in 5s...\")\n",
    "            if os.path.exists(\"handle.txt\"):\n",
    "                os.remove(\"handle.txt\")\n",
    "            await asyncio.sleep(5)\n",
    "\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f20f8272-8ef3-45b8-b4ad-c9736b30fb8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basketball is a **\n",
      "--------------------------------------------------------------------------------------\n",
      "setup_complete=None server_content=LiveServerContent(\n",
      "  model_turn=Content(\n",
      "    parts=[\n",
      "      Part(\n",
      "        text='Basketball is a **'\n",
      "      ),\n",
      "    ]\n",
      "  )\n",
      ") tool_call=None tool_call_cancellation=None usage_metadata=None go_away=None session_resumption_update=None\n",
      "team sport** played with a **ball** on a **rectangular court** with a **\n",
      "--------------------------------------------------------------------------------------\n",
      "setup_complete=None server_content=LiveServerContent(\n",
      "  model_turn=Content(\n",
      "    parts=[\n",
      "      Part(\n",
      "        text='team sport** played with a **ball** on a **rectangular court** with a **'\n",
      "      ),\n",
      "    ]\n",
      "  )\n",
      ") tool_call=None tool_call_cancellation=None usage_metadata=None go_away=None session_resumption_update=None\n",
      "hoop (basket)** at each end. The objective is to **score points by shooting the ball through the opponent's hoop**, while preventing them from doing the same. Players **dribble (bounce) the ball** to move with\n",
      "--------------------------------------------------------------------------------------\n",
      "setup_complete=None server_content=LiveServerContent(\n",
      "  model_turn=Content(\n",
      "    parts=[\n",
      "      Part(\n",
      "        text=\"hoop (basket)** at each end. The objective is to **score points by shooting the ball through the opponent's hoop**, while preventing them from doing the same. Players **dribble (bounce) the ball** to move with\"\n",
      "      ),\n",
      "    ]\n",
      "  )\n",
      ") tool_call=None tool_call_cancellation=None usage_metadata=None go_away=None session_resumption_update=None\n",
      " it and **pass it** to teammates. It's a fast-paced game emphasizing **skill, teamwork, and athleticism.**\n",
      "--------------------------------------------------------------------------------------\n",
      "setup_complete=None server_content=LiveServerContent(\n",
      "  model_turn=Content(\n",
      "    parts=[\n",
      "      Part(\n",
      "        text=\" it and **pass it** to teammates. It's a fast-paced game emphasizing **skill, teamwork, and athleticism.**\"\n",
      "      ),\n",
      "    ]\n",
      "  )\n",
      ") tool_call=None tool_call_cancellation=None usage_metadata=None go_away=None session_resumption_update=None\n",
      "None\n",
      "--------------------------------------------------------------------------------------\n",
      "setup_complete=None server_content=LiveServerContent(\n",
      "  generation_complete=True\n",
      ") tool_call=None tool_call_cancellation=None usage_metadata=None go_away=None session_resumption_update=None\n",
      "None\n",
      "--------------------------------------------------------------------------------------\n",
      "setup_complete=None server_content=LiveServerContent(\n",
      "  turn_complete=True\n",
      ") tool_call=None tool_call_cancellation=None usage_metadata=UsageMetadata(\n",
      "  prompt_token_count=16,\n",
      "  prompt_tokens_details=[\n",
      "    ModalityTokenCount(\n",
      "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
      "      token_count=16\n",
      "    ),\n",
      "  ],\n",
      "  response_token_count=97,\n",
      "  response_tokens_details=[\n",
      "    ModalityTokenCount(\n",
      "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
      "      token_count=97\n",
      "    ),\n",
      "  ],\n",
      "  total_token_count=113\n",
      ") go_away=None session_resumption_update=None\n"
     ]
    }
   ],
   "source": [
    "# Detects session go_away warnings and create again a session with previous resumption \n",
    "import asyncio\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import os\n",
    "\n",
    "client = genai.Client()\n",
    "MODEL = \"gemini-live-2.5-flash-preview\"\n",
    "handle = open(\"handle.txt\").read().strip() if os.path.exists(\"handle.txt\") else None\n",
    "\n",
    "\n",
    "\n",
    "async def main():\n",
    "    cfg = types.LiveConnectConfig(response_modalities=[\"TEXT\"],session_resumption = {\"handle\": handle} if handle else None)\n",
    "\n",
    "    \n",
    "    async with client.aio.live.connect(model=MODEL, config=cfg) as s:\n",
    "        msg = \"give a brief about basket ball game.\"\n",
    "        await s.send_client_content(turns={\"role\": \"user\", \"parts\": [{\"text\": msg}]})\n",
    "        \n",
    "        async for response in s.receive():\n",
    "            print(response.text)\n",
    "            print('--------------------------------------------------------------------------------------')\n",
    "            print(response)\n",
    "            if getattr(response, \"go_away\", None) :\n",
    "                print(\"\\n Session expiring soon. Time left:\", response.go_away.time_left)\n",
    "            if response.setup_complete:\n",
    "\n",
    "                new_handle = response.setup_complete.session.handle\n",
    "                print(\"session handle:\",new_handle)\n",
    "\n",
    "                with open(\"handle.txt\",\"w\") as f:\n",
    "                    f.write(new_handle)\n",
    "                break\n",
    "              \n",
    "    \n",
    "\n",
    "await main()\n",
    "\n",
    "\"\"\"\n",
    "Establishes a Gemini live session resuming previous context if possible.\n",
    "\n",
    "Sends a user prompt as a turn.\n",
    "\n",
    "Streams and prints response every chunk, including text and low-level debug info.\n",
    "\n",
    "Detects session go_away warnings and session setup completion.\n",
    "\n",
    "Saves session handle for later resumption.\n",
    "\n",
    "Tests basic live interaction, session lifecycle management, and state persistence via handle.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "295e9674-e945-46cb-acd8-796e6854083f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected to Gemini Live\n",
      "Basketball is a **\n",
      "team sport** where the main goal is to **shoot a ball through an elevated\n",
      " hoop (basket)** on your opponent's side of the court while **preventing them from doing the same** on yours.\n",
      "\n",
      "Teams of five players move the ball by **dribbling (bouncing it)** and **passing**\n",
      " it. You score points by making a shot, with different distances giving you different point values (usually 2 or 3 points). The team with the most points at the end of the game wins.\n",
      "\n",
      "It's a game of **skill\n",
      ", strategy, and teamwork**, involving running, jumping, shooting, and defending.\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# it is same as previous code, only text_delta(real time response display) is extra\n",
    "import asyncio\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "client = genai.Client()\n",
    "MODEL = \"gemini-live-2.5-flash-preview\"\n",
    "\n",
    "async def main():\n",
    "    async with client.aio.live.connect(\n",
    "        model=MODEL,\n",
    "        config=types.LiveConnectConfig(response_modalities=[\"TEXT\"])\n",
    "    ) as s:\n",
    "        print(\"Connected to Gemini Live\")\n",
    "\n",
    "        await s.send_client_content(turns={\"role\": \"user\", \"parts\": [{\"text\": \"Explain basketball briefly\"}]})\n",
    "\n",
    "      \n",
    "        async for r in s.receive():\n",
    "            if getattr(r, \"text_delta\", None):\n",
    "                print(r.text_delta, end=\"\", flush=True)\n",
    "            if getattr(r, \"response_completed\", None):\n",
    "                print(\"\\n Response complete.\")\n",
    "                break\n",
    "            print(r.text)\n",
    "\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "064d59d7-7722-43f8-bca3-3715bfe454d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Gemini Live. Waiting for go_away...\n",
      "Session expiring soon. Time left: 50s\n"
     ]
    }
   ],
   "source": [
    "# go away ( time left)\n",
    "import asyncio\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "client = genai.Client()\n",
    "MODEL = \"gemini-live-2.5-flash-preview\"\n",
    "\n",
    "async def main():\n",
    "    async with client.aio.live.connect(\n",
    "        model=MODEL,\n",
    "        config=types.LiveConnectConfig(response_modalities=[\"TEXT\"],\n",
    "        context_window_compression=types.ContextWindowCompressionConfig(\n",
    "            sliding_window=types.SlidingWindow()\n",
    "        ))\n",
    "    ) as s:\n",
    "        print(\"Connected to Gemini Live. Waiting for go_away...\")\n",
    "\n",
    "        async for r in s.receive():\n",
    "            if getattr(r, \"go_away\", None):\n",
    "                print(\"Session expiring soon. Time left:\", r.go_away.time_left)\n",
    "                break  \n",
    "\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efdf2f3d-055c-434d-9155-c43207115e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great\n",
      "! Do you have a preferred restaurant in mind, or would you like me to\n",
      " help you find one?\n",
      "None\n",
      "\n",
      " Generation complete!\n"
     ]
    }
   ],
   "source": [
    "# handling streaming response\n",
    "\n",
    "import asyncio\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "client = genai.Client()\n",
    "MODEL = \"gemini-live-2.5-flash-preview\"\n",
    "\n",
    "async def main():\n",
    "    cfg = types.LiveConnectConfig(response_modalities=[\"TEXT\"])\n",
    "    \n",
    "    async with client.aio.live.connect(model=MODEL, config=cfg) as session:\n",
    "        await session.send_client_content(\n",
    "            turns={\"role\": \"user\", \"parts\": [{\"text\": \"I want to order a burger and fries.\"}]}\n",
    "        )\n",
    "\n",
    "        async for r in session.receive():\n",
    "            print(r.text)\n",
    "            if getattr(r, \"text_delta\", None):\n",
    "                print(r.text_delta, end=\"\", flush=True)\n",
    "            \n",
    "            # Detect if the model has finished generating the response\n",
    "            if getattr(r.server_content, \"generation_complete\", None):\n",
    "                print(\"\\n Generation complete!\")\n",
    "                break\n",
    "\n",
    "await main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
